{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559d181c-574e-449a-b1a0-e8bdaa84fad1",
   "metadata": {},
   "source": [
    "# Getting started with PyTorch and CIFAR\n",
    "In this notebook, you can load a pretrained model and fine-tune it on the desired task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4e9a658-8f88-447d-b2be-03a0538f0ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6, 7\"\n",
    "os.environ['SCIPY_USE_PROPACK'] = \"True\"\n",
    " \n",
    "threads = \"64\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = threads\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = threads\n",
    "os.environ[\"MKL_NUM_THREADS\"] = threads\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = threads\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c176b09f-9b94-40b5-96cd-83244aced2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12d62b3-d03d-4a97-97fc-d3e65375849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9719ad69-1a2f-492c-832c-37f2d72acbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not mandatory\n",
    "import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffe1d9b-3eb1-483c-9a07-d92d68e60561",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331bb352-0f5d-4a5d-b84b-532eb6415dcd",
   "metadata": {},
   "source": [
    "Define transforms and load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d569f597-8bd5-4f72-9090-5e15083191b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/srv/newpenny/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a93a047a-74f6-4c4c-b238-d8b49b58a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'CIFAR100'\n",
    "data_path = os.path.join(data_dir, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a70a222-634e-414e-851a-c240dff32ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_config(dataset):\n",
    "    dataset_config = {\n",
    "        'CIFAR10': {'num_classes': 10, \n",
    "                    'input_ch': 3, \n",
    "                    'means': (0.424, 0.415, 0.384), \n",
    "                    'stds': (0.283, 0.278, 0.284)},\n",
    "        \n",
    "        'CIFAR100': {'num_classes': 100, \n",
    "                     'input_ch': 3, \n",
    "                     'means': (0.438, 0.418, 0.377), \n",
    "                     'stds': (0.300, 0.287, 0.294)}\n",
    "    }\n",
    "    return dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adf11ede-4903-4569-ac8a-588d2add7f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = get_dataset_config(dataset)\n",
    "\n",
    "means_ = dc[dataset]['means']\n",
    "stds_ = dc[dataset]['stds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4a79378-fd7f-4759-9e0d-f9b68e7445f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset, augment=False, seed=42, data_dir=''):\n",
    "    '''\n",
    "    dataset (str): choices=['CIFAR10', CIFAR100', 'ImageNet']\n",
    "    '''\n",
    "    dc = get_dataset_config(dataset)\n",
    "\n",
    "    means_ = dc[dataset]['means']\n",
    "    stds_ = dc[dataset]['stds']\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "            #transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.CenterCrop((224, 224)),\n",
    "            #transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(means_, stds_)\n",
    "        ])\n",
    "\n",
    "    if dataset=='ImageNet':\n",
    "        data_path = os.path.join(data_dir, 'imagenet-1k/data')\n",
    "    elif dataset.startswith('CIFAR'):\n",
    "        data_path = os.path.join(data_dir, 'imagenet-1k/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ad5e083-4c5f-4560-8b24-932e294f5675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.startswith('CIFAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9ede87-7897-4e84-8a74-fc27253ab154",
   "metadata": {},
   "source": [
    "Decide if you want to perform augmentation on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93c470ea-9dec-4a95-bf54-99face25daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ff28da5-e5d2-48b7-aedf-abf0f5318b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "            #transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.CenterCrop((224, 224)),\n",
    "            #transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(means_, stds_)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3075d5-d8f0-4f49-a365-fcf78e0f9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trials imagenet-1k\n",
    "# data_path = os.path.join(data_dir, 'imagenet-1k/data')\n",
    "# data_path\n",
    "\n",
    "# in_dataset = torchvision.datasets.ImageNet(root=data_path, \n",
    "#                                            split='train', \n",
    "#                                            transform=transform)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42cc48a-65e0-4e39-a47f-6f47ef29aecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR100\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_dataset = torchvision.datasets.__dict__[dataset](root=data_path, \n",
    "                                                       train=True, \n",
    "                                                       transform=transform, \n",
    "                                                       download=True)\n",
    "\n",
    "# split the dataset into train and validation sets\n",
    "train_size = int(0.8 * len(cifar_dataset))\n",
    "val_size = len(cifar_dataset) - train_size\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(cifar_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "# create DataLoaders\n",
    "batch_size = 64\n",
    "num_workers = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f6701-09a5-41f6-be00-cd32c33429f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if augment:\n",
    "    transform_ = transforms.Compose([\n",
    "                #transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
    "                transforms.CenterCrop((224, 224)),\n",
    "                #transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(means_, stds_)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c704b-5b66-4996-bdfd-fe39c08b53bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_dataset = torchvision.datasets.__dict__[dataset](root=data_path, \n",
    "                                                       train=True, \n",
    "                                                       transform=transform_, \n",
    "                                                       download=True)\n",
    "\n",
    "# split the dataset into train and validation sets\n",
    "train_size = int(0.8 * len(cifar_dataset))\n",
    "val_size = len(cifar_dataset) - train_size\n",
    "train_dataset, _ = random_split(cifar_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "# create DataLoaders\n",
    "batch_size = 64\n",
    "num_workers = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c97a3-5771-41da-a1e8-73500fcbe6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.__dict__[dataset](root=data_path, \n",
    "                                                      train=False, \n",
    "                                                      download=True, \n",
    "                                                      transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3df3df-0b8b-4e90-8eb6-d4a0fdc5becc",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d431b6-27c2-493f-8b81-c768b0c8f6e6",
   "metadata": {},
   "source": [
    "Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d06fc4-df71-4e08-af9a-654a044632ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resnet50'\n",
    "#model = torchvision.models.resnet50(pretrained=True) # this will raise a warning\n",
    "model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3a403-3d8c-4d61-b7f6-cf69a78ef7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'vgg16'\n",
    "#model = torchvision.models.__dict__[model_name](pretrained=True) # this will raise a warning\n",
    "model = torchvision.models.__dict__[model_name](weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4c50d9-bd07-417b-8262-f2caff9900b9",
   "metadata": {},
   "source": [
    "Check the architecture of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6826e5-8bda-47f5-8aab-85db9ceffcb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c392255-aeb4-4721-9e70-d4c9e90d1e86",
   "metadata": {},
   "source": [
    "Change shape of last layer if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292cf664-1d1a-4439-9ad4-1ad59c281150",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(cifar_dataset.classes)\n",
    "\n",
    "if model_name == 'resnet50':\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(in_features, n_classes)\n",
    "elif model_name == 'vgg16':\n",
    "    in_features = model.classifier[-1].in_features\n",
    "    model.classifier[-1] = torch.nn.Linear(in_features, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff6638-722e-46c1-864a-f86b71dae422",
   "metadata": {},
   "source": [
    "Model parameters are saved in a `state_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60aefd0-a561-4bd4-bbcc-ac4392455922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cad2a28-471f-445a-97e3-9568f011c464",
   "metadata": {},
   "source": [
    "You can access the keys of the `state_dict` to get the names of layers with parameters (this can be useful to identify layer names and allow for customization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a106d-e4b2-4222-9232-ac2275779648",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0fd2f-d5c1-44ef-85c7-4c96464b8f34",
   "metadata": {},
   "source": [
    "## Training with early stopping and learning rate scheduling\n",
    "The choice of the parameters reported below strongly affects the quality of the training process. Decomment to try different settings. You will need to find a proper combination to obtain a model performing with desirable properties &#128522;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc477e5-af58-4232-90d8-0bcf08bec08f",
   "metadata": {},
   "source": [
    "### Parameter settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ebb5c-ff7d-4d8f-8432-f9f795ca46d6",
   "metadata": {},
   "source": [
    "Number of epochs choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ea34c-57bd-432b-bf21-ab7ffc25a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9159f1a5-eff1-4f8e-a835-db77b271d617",
   "metadata": {},
   "source": [
    "Loss function and optimizer initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadace87-5870-45a3-8cd7-e825852fc573",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f655303-b4a7-4ca5-869a-a3c6a23606bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 0.001\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=0.9)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=initial_lr, weight_decay=1e-4) # more computationally intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178006d5-4d99-4587-9f2c-1c89322552b5",
   "metadata": {},
   "source": [
    "Early stopping parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04698f3-9c7c-452e-a332-d918530c78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_patience = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ab90f-9025-42c0-826d-15f38d65ef2e",
   "metadata": {},
   "source": [
    "Learning rate scheduling\\\n",
    "Notes on usage:\n",
    "* Schedulers like `StepLR`: Decay the learning rate at regular intervals, regardless of validation performance. They are simpler and can be effective for general tasks.\n",
    "* `ReduceLROnPlateau`: Adapts the learning rate based on validation loss, which can be especially useful in fine-tuning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd364c5-2d83-4b91-88b0-9f1f595ef8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "#scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "lr_patience = 5\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=lr_patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60250463-6bc6-4db6-9bcb-955bdbd3c163",
   "metadata": {},
   "source": [
    "You can choose the performance metric you want to optimize during training (typically, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a451d0a1-0472-4db4-a20a-fb2d118315af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, targets):\n",
    "    _, predicted = torch.max(outputs, 1)  # get the class index with the highest probability\n",
    "    correct = (predicted == targets).sum().item()\n",
    "    total = targets.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5acbc0-d6fc-4701-8a13-36d872381c1f",
   "metadata": {},
   "source": [
    "GPU selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4384fc-1383-457a-bd5b-834223be3ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "cuda_index = torch.cuda.device_count() - 1\n",
    "device = torch.device(f\"cuda:{cuda_index}\" if use_cuda else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd058157-093e-47e1-aa3c-55128006c4ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a95254-74fa-4adb-a5a7-51ce36d1894a",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666cdef2-95b5-46ee-83d5-4bd37130bdd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patience_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "tl = []\n",
    "vl = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for data in tqdm(train_loader):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()        \n",
    "        \n",
    "        correct_predictions += accuracy(outputs, targets) * targets.size(0)\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # compute validation loss\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, targets = data\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            val_correct_predictions += accuracy(outputs, targets) * targets.size(0)\n",
    "            val_total_predictions += targets.size(0)\n",
    "            \n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    tl.append(train_loss)\n",
    "    vl.append(val_loss)\n",
    "    \n",
    "    train_accuracy = (correct_predictions / total_predictions) * 100\n",
    "    val_accuracy = (val_correct_predictions / val_total_predictions) * 100\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    # early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping: Validation loss hasn't improved for\", early_stopping_patience, \"epochs.\")\n",
    "            break\n",
    "\n",
    "    # step the scheduler\n",
    "    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "        scheduler.step(val_loss)\n",
    "    else:\n",
    "        scheduler.step()\n",
    "\n",
    "    current_lr = scheduler.get_last_lr()[0] if hasattr(scheduler, 'get_last_lr') else optimizer.param_groups[0]['lr']\n",
    "    print(f'Current lr: {current_lr:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dfaa2e-9f3a-4261-b662-9f77f2d1ebd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(tl, label='train loss')\n",
    "plt.plot(vl, label='val loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8606e53-9762-48fd-a9fd-b038b552e0cb",
   "metadata": {},
   "source": [
    "## Save model checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb2282e-15b4-4c21-b5db-50712b47c951",
   "metadata": {},
   "source": [
    "Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72c337-21da-45bb-9955-2f41e2bb27c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = '/srv/newpenny/XAI/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd8671-c689-4d26-99ce-e5a7c060d2ee",
   "metadata": {},
   "source": [
    "Add your initials to model name, e.g.:\n",
    "```python\n",
    "my_initials = 'LM'\n",
    "model_path = f'{my_initials}_etc.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f77fd2-10b9-4518-aac2-47a3c8b198ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = '/srv/newpenny/XAI/models'\n",
    "my_initials = 'LM' # use yours!\n",
    "model_name =  model_name # should be already defined\n",
    "dataset_name = dataset # should be already defined\n",
    "other_params = 'augment=True_optim=SGD_scheduler=LROnPlateau' # 'make_string_of_other_relevant_params' # can be optimizer, epochs, final lr...\n",
    "\n",
    "path = f'{my_initials}_model={model_name}_dataset={dataset_name}_{other_params}.pth'\n",
    "model_path = os.path.join(models_dir, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9745b5-75fe-4558-ae3f-d12074e4ee2a",
   "metadata": {},
   "source": [
    "### Basic version: `model.state_dict()` only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b1bf89-e3d8-4f6e-93d2-b204dbe7b2c7",
   "metadata": {},
   "source": [
    "Remember that `model.state_dict()` will contain all parameters learned after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a838d-5d81-4963-b804-dac273fc4fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomment to save\n",
    "# torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b0fa9-d981-4d3f-b7b6-22b903ab5f2b",
   "metadata": {},
   "source": [
    "You can then load the learned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb4030a-76cd-4e66-9acf-fb01b94b8d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Net() # instantiate the kind of model\n",
    "# model.load_state_dict(torch.load(model_path, weights_only=True)) # load saved parameters (weights and biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d35f4f-9139-4e26-a20b-5d4bbe411abd",
   "metadata": {},
   "source": [
    "### Checkpoint with info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd5337-c148-4f3e-8e55-529129e6d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'state_dict' : model.state_dict(),\n",
    "    'epoch': epoch,\n",
    "    'initial_lr' : initial_lr,\n",
    "    'final_lr': current_lr,\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'val_accuracy': val_accuracy\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93137c84-32f7-4c03-a891-bbd4e1626d03",
   "metadata": {},
   "source": [
    "Eventually add a loss plot for quick visualization of the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf4f9f-f16f-49b3-ad2f-1c722c4464e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(tl, label='train loss')\n",
    "plt.plot(vl, label='val loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "buf = io.BytesIO()\n",
    "plt.savefig(buf, format='PNG')\n",
    "buf.seek(0)\n",
    "checkpoint['loss_plot'] = buf.getvalue() \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b1100-17c5-428d-bc22-3943503ee475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomment to save\n",
    "torch.save(checkpoint, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281396c3-98f3-49bd-9e92-93c1e8ea2134",
   "metadata": {},
   "source": [
    "You can then load the learned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898a2873-9f53-4497-bed9-d103d1081567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = Net() # decomment and instantiate the kind of model you want\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint['state_dict']) # load saved parameters (weights and biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c6fc3-3847-44e7-9265-c37eb05295b0",
   "metadata": {},
   "source": [
    "If you saved a loss plot as suggested above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ff266-ea16-4615-b383-b2a5f1ac04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the image bytes and display it\n",
    "image_bytes = checkpoint['loss_plot']\n",
    "image = Image.open(io.BytesIO(image_bytes))\n",
    "image.show()  # this will open the image in the default image viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8815e3-fabb-4bf8-99bf-95714d298463",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# alternatively, display the image inline (if in a notebook or interactive environment)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766ce81-9740-4d7d-bca2-bb1c05ccf251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
