{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ViT with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to train the Visual Transformer on CIFAR100\n",
    "\n",
    "Work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "# gpu selection\n",
    "use_cuda = torch.cuda.is_available()\n",
    "cuda_index = 2  # torch.cuda.device_count() -1\n",
    "device = torch.device(f\"cuda:{cuda_index}\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset Cifar 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_config(dataset):\n",
    "    dataset_config = {\n",
    "        'CIFAR10': {'num_classes': 10, \n",
    "                    'input_ch': 3, \n",
    "                    'means': (0.424, 0.415, 0.384), \n",
    "                    'stds': (0.283, 0.278, 0.284)},\n",
    "        \n",
    "        'CIFAR100': {'num_classes': 100, \n",
    "                     'input_ch': 3, \n",
    "                     'means': (0.438, 0.418, 0.377), \n",
    "                     'stds': (0.300, 0.287, 0.294)},\n",
    "        \n",
    "        'ImageNet': {'num_classes': 1000, \n",
    "                     'input_ch': 3,\n",
    "                     'means': [0.485, 0.456, 0.406],\n",
    "                     'stds': [0.229, 0.224, 0.225]}\n",
    "    }\n",
    "    return dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(**kwargs):\n",
    "def load_data(dataset, split, augment=False, shuffle_train=False, batch_size=64, num_workers=8, seed=42, data_dir='/srv/newpenny/dataset'):\n",
    "    '''\n",
    "    dataset (str): choices=['CIFAR10', CIFAR100', 'ImageNet']\n",
    "    '''\n",
    "    dc = get_dataset_config(dataset)\n",
    "\n",
    "    means_ = dc[dataset]['means']\n",
    "    stds_ = dc[dataset]['stds']\n",
    "   \n",
    "    if augment and split=='train': # acts only on the training set\n",
    "        transform = transforms.Compose([\n",
    "            #transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
    "            transforms.CenterCrop((224, 224)),\n",
    "            #transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(means_, stds_)\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            #transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.CenterCrop((224, 224)),\n",
    "            #transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(means_, stds_)\n",
    "        ])\n",
    "          \n",
    "    if dataset=='ImageNet':\n",
    "        # NB. the loader doesn't work for the 'test' split\n",
    "        data_path = os.path.join(data_dir, 'imagenet-1k/data')\n",
    "        data = torchvision.datasets.__dict__[dataset](root=data_path, \n",
    "                                                      split=split, \n",
    "                                                      transform=transform)\n",
    "    elif dataset.startswith('CIFAR'):\n",
    "        data_path = os.path.join(data_dir, dataset)\n",
    "        if split!='test': # data for train and val\n",
    "            data = torchvision.datasets.__dict__[dataset](root=data_path, \n",
    "                                                        train=True, \n",
    "                                                        transform=transform, \n",
    "                                                        download=True)\n",
    "        else:\n",
    "            data = torchvision.datasets.__dict__[dataset](root=data_path, \n",
    "                                                        train=False, \n",
    "                                                        transform=transform, \n",
    "                                                        download=True)\n",
    "\n",
    "    if split=='test':\n",
    "        loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    else:\n",
    "        # split the dataset into train and validation sets\n",
    "        train_size = int(0.8 * len(data))\n",
    "        val_size = len(data) - train_size\n",
    "        generator = torch.Generator().manual_seed(42)\n",
    "        train_data, val_data = random_split(data, [train_size, val_size], generator=generator)\n",
    "        if split=='train':\n",
    "            loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=shuffle_train, num_workers=num_workers)\n",
    "        else:\n",
    "            loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/srv/newpenny/dataset'\n",
    "\n",
    "dataset = 'CIFAR100'\n",
    "\n",
    "train_loader = load_data(dataset, \"train\", augment=True)\n",
    "val_loader = load_data(dataset, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_loader.dataset.dataset.classes)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Weights of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'vit_b_16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'resnet50':\n",
    "    weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V1\n",
    "elif model_name == 'vgg16':\n",
    "    weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1\n",
    "elif model_name == 'vit_b_16':\n",
    "    weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1\n",
    "\n",
    "model = torchvision.models.__dict__[model_name](weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show architecture of the model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the shape of the last layer from 1000 to 100 -> `out_features=100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(train_loader.dataset.dataset.classes)\n",
    "\n",
    "if model_name == 'resnet50':\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(in_features, n_classes)\n",
    "elif model_name == 'vgg16':\n",
    "    in_features = model.classifier[-1].in_features\n",
    "    model.classifier[-1] = torch.nn.Linear(in_features, n_classes)\n",
    "elif model_name == 'vit_b_16':\n",
    "    in_features = model.heads.head.in_features\n",
    "    model.heads.head = torch.nn.Linear(in_features, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "initial_lr = 0.001\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=0.9)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=initial_lr, weight_decay=1e-4) # more computationally intensive\n",
    "\n",
    "early_stopping_patience = 10\n",
    "\n",
    "#scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "#scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "lr_patience = 5\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=lr_patience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, targets):\n",
    "    _, predicted = torch.max(outputs, 1)  # get the class index with the highest probability\n",
    "    correct = (predicted == targets).sum().item()\n",
    "    total = targets.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU selection\n",
    "use_cuda = torch.cuda.is_available()\n",
    "cuda_index = torch.cuda.device_count() - 1\n",
    "device = torch.device(f\"cuda:{cuda_index}\" if use_cuda else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard\n",
    "save_path = os.path.join(os.path.expanduser(\"~\"), \"Documents\", \"runs\")\n",
    "writer = SummaryWriter(save_path)\n",
    "save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "tl = []\n",
    "vl = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for data in tqdm(train_loader):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()        \n",
    "        \n",
    "        correct_predictions += accuracy(outputs, targets) * targets.size(0)\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # compute validation loss\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, targets = data\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            val_correct_predictions += accuracy(outputs, targets) * targets.size(0)\n",
    "            val_total_predictions += targets.size(0)\n",
    "            \n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    tl.append(train_loss)\n",
    "    vl.append(val_loss)\n",
    "    \n",
    "    train_accuracy = (correct_predictions / total_predictions) * 100\n",
    "    val_accuracy = (val_correct_predictions / val_total_predictions) * 100\n",
    "    \n",
    "\t#tensorboard\n",
    "    writer.add_scalar('train loss',train_loss,epoch)\n",
    "    writer.add_scalar('train accuracy',train_accuracy,epoch)\n",
    "    writer.add_scalar('val loss',val_loss,epoch)\n",
    "    writer.add_scalar('val accuracy',val_accuracy,epoch)\n",
    "    writer.add_scalar('lr',optimizer.param_groups[0]['lr'],epoch)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    # early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping: Validation loss hasn't improved for\", early_stopping_patience, \"epochs.\")\n",
    "            break\n",
    "\n",
    "    # step the scheduler\n",
    "    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "        scheduler.step(val_loss)\n",
    "    else:\n",
    "        scheduler.step()\n",
    "\n",
    "    current_lr = scheduler.get_last_lr()[0] if hasattr(scheduler, 'get_last_lr') else optimizer.param_groups[0]['lr']\n",
    "    print(f'Current lr: {current_lr:.6f}')\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
