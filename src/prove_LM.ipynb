{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3a5877-a6d8-4946-a65f-6aeae8f68405",
   "metadata": {},
   "source": [
    "# New peepholes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1fb98-07ee-4780-bd4a-da11d16e509c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prime prove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5706e3d-584e-4606-be81-de4681ed68c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensordict import TensorDict\n",
    "from tensordict import MemoryMappedTensor as MMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d24335-e230-4295-a83e-1f6362062a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c9b5e-e7ba-4407-ad0f-a4fd13cafbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python stuff\n",
    "from pathlib import Path as Path\n",
    "from numpy.random import randint\n",
    "\n",
    "# Our stuff\n",
    "from datasets.cifar import Cifar\n",
    "from models.model_wrap import ModelWrap \n",
    "from peepholes.peepholes import Peepholes\n",
    "from peepholes.svd_peepholes import peep_matrices_from_svds as parser_fn\n",
    "\n",
    "# torch stuff\n",
    "import torch\n",
    "from torchvision.models import vgg16, VGG16_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184db83-28d1-4d96-b565-c1070fee25b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#import datetime\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6769c-52eb-4b44-9976-1ca6fd407fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clustering.clustering import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5eb95f-0952-41c8-ad07-d0a40a8ce6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: cambiare i path (anche nei moduli)\n",
    "p_dir = os.path.join('../data/peepholes')\n",
    "svd_dir = os.path.join('../data/svds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462aead-ad9d-4893-82af-75ce96c8cdbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4950ac12-9ac5-4778-a094-829f95cd770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(svd_dir, 'svds')\n",
    "svds = TensorDict.load_memmap(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2fa597-a59f-42d5-8dac-1dd9b19dfb1e",
   "metadata": {},
   "source": [
    "### Singular Values visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64767f5-61b1-488d-aed9-bb23b529882f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "for i, layer in enumerate(svds.keys()):\n",
    "    axs.plot(svds[layer]['s'], label=layer, alpha=0.7)\n",
    "axs.set_xlim(-2, 300)\n",
    "axs.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213391cc-533b-43a0-9a31-efba44281d4b",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5f83b-fadb-4983-8a72-5c7a4ecb4f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# Dataset \n",
    "#--------------------------------\n",
    "# model parameters\n",
    "dataset = 'CIFAR10' \n",
    "seed = 29\n",
    "bs = 64\n",
    "\n",
    "ds_path = f'/srv/newpenny/dataset/{dataset}'\n",
    "ds = Cifar(data_path=ds_path,\n",
    "           dataset=dataset)\n",
    "\n",
    "ds.load_data(\n",
    "        batch_size = bs,\n",
    "        data_kwargs = {'num_workers': 4, 'pin_memory': True},\n",
    "        seed = seed,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fe42ce-b161-4599-bf92-c13bf238da97",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584beec1-909f-4f35-b5ad-f5e133b1fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "cuda_index = torch.cuda.device_count() - 2\n",
    "device = torch.device(f\"cuda:{cuda_index}\" if use_cuda else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e59cd3-bd75-4923-96a2-75a1831844db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = True\n",
    "model_dir = '/srv/newpenny/XAI/LM/models'\n",
    "model_name = f'vgg16_pretrained={pretrained}_dataset={dataset}-'\\\n",
    "f'augmented_policy=CIFAR10_bs={bs}_seed={seed}.pth'\n",
    "\n",
    "\n",
    "nn = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "in_features = 4096\n",
    "num_classes = len(ds.get_classes()) \n",
    "nn.classifier[-1] = torch.nn.Linear(in_features, num_classes)\n",
    "model = ModelWrap(device=device)\n",
    "model.set_model(model=nn, path=model_dir, name=model_name, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652471d6-5384-477c-8fb9-e4f8f282833f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load peepholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da3340-a8d0-4f3c-ad0c-42a7eed7f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_path = '/srv/newpenny/XAI/generated_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c34548-149f-4d93-801c-c94274fffeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "phs_name = 'peepholes'\n",
    "phs_dir = os.path.join(abs_path, 'peepholes')\n",
    "peepholes = Peepholes(\n",
    "        path = phs_dir,\n",
    "        name = phs_name,\n",
    "        )\n",
    "loaders = ds.get_dataset_loaders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ddd632-551a-4cd6-b882-2cdc7935f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy dataset to peepholes dataset\n",
    "peepholes.get_peep_dataset(\n",
    "        loaders = loaders,\n",
    "        verbose = True\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9012ee88-9770-4f64-91a1-daf8acdbb473",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_dl = peepholes.get_dataloaders(batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f7f80-41c1-4a6e-b28a-90581a34cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dict = {'classifier': [0, 3],\n",
    "               'features': [24, 26, 28]}\n",
    "model.set_target_layers(target_layers=layers_dict, verbose=True)\n",
    "print('target layers: ', model.get_target_layers().keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a53c8e-b37a-4313-b667-8e9c31cbf732",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_list = list(model.get_target_layers().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ed96f-db70-4c08-8440-3e8116d7f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_layers = list(next(iter(ph_dl['train']))['peepholes'].keys())\n",
    "#list(next(iter(ph_dl['train']))['peepholes'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf5906b-3e95-4636-bbff-5985cf37134e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Clustering class\n",
    "\n",
    "* fit on training data\n",
    "    * can obtain k from the data shape \n",
    "* the Class can provide up to the output_peephole per layer. then we will use dedicated functions to get:\n",
    "    * quantities per single layer (max, entropy)\n",
    "    * combined scores with multiple layers (simple weights and exponentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472782a6-509a-4a38-9531-0537da43ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_kwargs = {}\n",
    "\n",
    "layer = 'classifier-3'\n",
    "\n",
    "clust_kwargs[layer]= {'algorithm': 'kmeans',\n",
    "                      'k': 100,\n",
    "                      'n': num_classes,\n",
    "                       }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d3533a-3584-46e4-931c-6f2e2ad1fa17",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Clustering: # quella buona\n",
    "    def __init__(self, algorithm, k, n_clusters, seed=42, \n",
    "                 base_dir='clustering'):\n",
    "        self.algorithm = algorithm  \n",
    "        self.k = k  \n",
    "        self.n_clusters = n_clusters\n",
    "        self.seed = seed\n",
    "        self.base_dir = f'/srv/newpenny/XAI/generated_data/{base_dir}'\n",
    "\n",
    "        self._fitted_model = None\n",
    "        self._cluster_assignments = None        # cluster assignments from the model\n",
    "        self._cluster_centers = None            # cluster centers from the model\n",
    "        self._cluster_covariances = None        # cluster covariances from the model\n",
    "        self._empirical_posteriors = None       # empirical posteriors (P(g, c))\n",
    "\n",
    "    def fit(self, core_vectors, labels=None):\n",
    "        '''\n",
    "        Perform clustering on the training core_vectors of a specific layer.\n",
    "        \n",
    "        Args:\n",
    "        - core_vectors (Tensor): Ex-\"peepholes\" with reduced dimension (n_samples, k)\n",
    "        - labels (Tensor): Labels for empirical posteriors computation (optional)\n",
    "        '''\n",
    "        #if self.algorithm == 'gmm':\n",
    "        #    model = GaussianMixture(n_components=self.n_clusters, random_state=self.seed)\n",
    "        #    model.fit(core_vectors)\n",
    "        #    self._cluster_assignments = model.predict(core_vectors)\n",
    "        #    self._cluster_centers = model.means_\n",
    "        #    self._cluster_covariances = model.covariances_\n",
    "        #    self._fitted_model = model\n",
    "        #    \n",
    "        #elif self.algorithm == 'kmeans':\n",
    "        #    model = KMeans(n_clusters=self.n_clusters, random_state=self.seed)\n",
    "        #    model.fit(core_vectors)\n",
    "        #    self._cluster_assignments = model.predict(core_vectors)\n",
    "        #    self._cluster_centers = model.cluster_centers_\n",
    "        #    self._fitted_model = model\n",
    "#\n",
    "        ## compute empirical posteriors if labels are provided\n",
    "        #if labels is not None:\n",
    "        #    self.compute_empirical_posteriors(labels)\n",
    "        if self.algorithm == 'gmm':\n",
    "            model = GaussianMixture(n_components=self.n_clusters, random_state=self.seed)\n",
    "            model.fit(core_vectors)\n",
    "            self._cluster_assignments = model.predict(core_vectors)\n",
    "            self._cluster_centers = model.means_\n",
    "            self._cluster_covariances = model.covariances_\n",
    "            self._fitted_model = model\n",
    "    \n",
    "        elif self.algorithm == 'kmeans':\n",
    "            model = KMeans(n_clusters=self.n_clusters, random_state=self.seed)\n",
    "            model.fit(core_vectors)\n",
    "            self._cluster_assignments = model.predict(core_vectors)\n",
    "            self._cluster_centers = model.cluster_centers_\n",
    "            self._fitted_model = model\n",
    "    \n",
    "        # Check if clustering was successful\n",
    "        if self._cluster_assignments is None:\n",
    "            raise ValueError(\"Clustering failed. No assignments were generated.\")\n",
    "    \n",
    "        # Compute empirical posteriors if labels are provided\n",
    "        if labels is not None:\n",
    "            self.compute_empirical_posteriors(labels)\n",
    "\n",
    "    def compute_empirical_posteriors(self, labels):\n",
    "        '''\n",
    "        Compute the empirical posterior matrix P, where P(g, c) is the probability\n",
    "        that a sample assigned to cluster g belongs to class c.\n",
    "\n",
    "        Args:\n",
    "        - labels (Tensor): True class labels for the samples (n_samples, )\n",
    "        '''\n",
    "        n_samples = len(labels)\n",
    "        n_classes = len(torch.unique(labels))\n",
    "        \n",
    "        # initialize matrix to count occurrences of (cluster g, class c) pairs\n",
    "        P_counts = torch.zeros(self.n_clusters, n_classes)\n",
    "\n",
    "        # count occurrences of (cluster g, class c) pairs\n",
    "        for i in range(n_samples):\n",
    "            c = int(labels[i].item())  # true class label\n",
    "            g = int(self._cluster_assignments[i])  # cluster assignment\n",
    "            P_counts[g, c] += 1\n",
    "\n",
    "        # normalize to get empirical posteriors\n",
    "        P_empirical = P_counts / P_counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # nandle potential division by zero\n",
    "        P_empirical = torch.nan_to_num(P_empirical)  # replace NaN with 0\n",
    "\n",
    "        self._empirical_posteriors = P_empirical\n",
    "\n",
    "    def cluster_probabilities(self, core_vectors):\n",
    "        '''\n",
    "        Get cluster probabilities for the provided core_vectors based on the fitted model.\n",
    "        \n",
    "        Args:\n",
    "        - core_vectors (Tensor): Peepholes with reduced dimension (n_samples, k)\n",
    "        \n",
    "        Returns:\n",
    "        - cluster_probs (Tensor): Probabilities for each cluster (n_samples, n_clusters)\n",
    "        '''\n",
    "        if self.algorithm == 'gmm':\n",
    "            return self._fitted_model.predict_proba(core_vectors)  # (n_samples, n_clusters)\n",
    "        elif self.algorithm == 'kmeans':\n",
    "            # get distances to each cluster center\n",
    "            distances = self._fitted_model.transform(core_vectors)\n",
    "            distances = torch.tensor(distances)\n",
    "            # convert distances to probabilities (soft assignment)\n",
    "            cluster_probs = torch.exp(-distances ** 2 / (2 * (distances.std() ** 2)))  # Gaussian-like softmax\n",
    "            cluster_probs = cluster_probs / cluster_probs.sum(dim=1, keepdim=True)  # normalize to probabilities\n",
    "            return cluster_probs\n",
    "\n",
    "    def map_clusters_to_classes(self, core_vectors):\n",
    "        '''\n",
    "        Map the cluster probabilities to class probabilities using empirical posteriors.\n",
    "        \n",
    "        Args:\n",
    "        - cluster_probs (Tensor): Probabilities for each cluster (n_samples, n_clusters)\n",
    "        \n",
    "        Returns:\n",
    "        - class_probs (Tensor): Probabilities for each class (n_samples, n_classes)\n",
    "        '''\n",
    "        if self._empirical_posteriors is None:\n",
    "            raise RuntimeError('Please run compute_empirical_posteriors() first.')\n",
    "\n",
    "        cluster_probs = self.cluster_probabilities(core_vectors)\n",
    "        cluster_probs = torch.tensor(cluster_probs, dtype=torch.float32)\n",
    "        \n",
    "        class_probs = torch.matmul(cluster_probs, self._empirical_posteriors)  # shape: (n_samples, n_classes)\n",
    "        class_probs = class_probs / class_probs.sum(dim=1, keepdim=True) # aka the new peepholes\n",
    "        return class_probs\n",
    "\n",
    "    # confidence scores ---------------------\n",
    "    #def get_confidence_scores(self, class_probs, score_type=\"max\"):\n",
    "    #    '''\n",
    "    #    Compute confidence scores (either max or entropy), save them, or load if they already exist.\n",
    "    #    \n",
    "    #    Args:\n",
    "    #    - class_probs (Tensor): Probabilities for each class (n_samples, n_classes)\n",
    "    #    - score_type (str): Either 'max' or 'entropy' to specify the type of confidence score.\n",
    "    #    \n",
    "    #    Returns:\n",
    "    #    - confidence_scores (Tensor): The computed confidence scores (n_samples, )\n",
    "    #    '''\n",
    "    #    confidence_dir = os.path.join(self.base_dir, 'confidence_scores')\n",
    "    #    filepath = self.construct_filepath(prefix=f\"cs={score_type}\", suffix=\"pkl\", dir_path=confidence_dir)\n",
    "    #    \n",
    "    #    if os.path.exists(filepath):\n",
    "    #        print(f\"Confidence scores ({score_type}) already exist at {filepath}. Loading...\")\n",
    "    #        with open(filepath, 'rb') as f:\n",
    "    #            confidence_scores = pickle.load(f)\n",
    "    #        return confidence_scores\n",
    "    #\n",
    "    #    if score_type == \"max\":\n",
    "    #        confidence_scores = torch.max(class_probs, dim=1).values\n",
    "    #    elif score_type == \"entropy\":\n",
    "    #        # entropy: -sum(p * log(p)) across classes (axis 1)\n",
    "    #        entropy = -torch.sum(class_probs * torch.log(class_probs + 1e-12), dim=1)\n",
    "    #        confidence_scores = entropy\n",
    "    #    else:\n",
    "    #        raise ValueError(f\"Invalid score_type: {score_type}. Use 'max' or 'entropy'.\")\n",
    "    #\n",
    "    #    os.makedirs(confidence_dir, exist_ok=True)\n",
    "    #    with open(filepath, 'wb') as f:\n",
    "    #        pickle.dump(confidence_scores, f)\n",
    "    #    print(f'Confidence scores ({score_type}) saved to {filepath}')\n",
    "    #\n",
    "    #    return confidence_scores\n",
    "    def get_confidence_scores(self, class_probs, split='train', score_type='max', save=False):\n",
    "        #confidence_dir = os.path.join(self.base_dir, 'confidence_scores')\n",
    "        #os.makedirs(confidence_dir, exist_ok=True)\n",
    "#\n",
    "        #filename = \"_\".join([f'cs={score_type}', self.construct_filename()])\n",
    "        #filepath = os.path.join(confidence_dir, split, filename)\n",
    "#\n",
    "        #if os.path.exists(filepath):\n",
    "        #    print(f\"Confidence scores ({score_type}) already exist at {filepath}. Loading...\")\n",
    "        #    with open(filepath, 'rb') as f:\n",
    "        #        confidence_scores = pickle.load(f)\n",
    "        #    return confidence_scores\n",
    "\n",
    "        if score_type == \"max\":\n",
    "            confidence_scores = torch.max(class_probs, dim=1).values\n",
    "        elif score_type == \"entropy\":\n",
    "            entropy = -torch.sum(class_probs * torch.log(class_probs + 1e-12), dim=1)\n",
    "            confidence_scores = entropy\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid score_type: {score_type}. Use 'max' or 'entropy'.\")\n",
    "        #if save:\n",
    "        #    with open(filepath, 'wb') as f:\n",
    "        #        pickle.dump(confidence_scores, f)\n",
    "        #    print(f'Confidence scores ({score_type}) saved to {filepath}')\n",
    "\n",
    "        return confidence_scores\n",
    "\n",
    "\n",
    "\n",
    "    # save/load results ---------------------\n",
    "    def save_cluster_results(self, filepath=None):\n",
    "        \"\"\"\n",
    "        Save the clustering results (assignments, centers, covariances) to a file.\n",
    "        \"\"\"\n",
    "        if filepath is None:\n",
    "            filepath = self.construct_filepath(suffix='pkl') \n",
    "\n",
    "        data = {\n",
    "            'assignments': self._cluster_assignments,\n",
    "            'centers': self._cluster_centers,\n",
    "            'k': self.k,\n",
    "        }\n",
    "\n",
    "        if self.algorithm == 'gmm':\n",
    "            data['covariances'] = self._cluster_covariances\n",
    "\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "        # print(f'Clustering results saved to {filepath}')\n",
    "\n",
    "    def load_cluster_results(self, filepath=None):\n",
    "        \"\"\"\n",
    "        Load clustering results from a file.\n",
    "        \"\"\"\n",
    "        if filepath is None:\n",
    "            filepath = self.construct_filepath(suffix='pkl') \n",
    "\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                results = pickle.load(f)\n",
    "            self._cluster_assignments = results['assignments']\n",
    "            self._cluster_centers = results['centers']\n",
    "            self.k = results.get('k', self.k)\n",
    "\n",
    "            if self.algorithm == 'gmm' and 'covariances' in results:\n",
    "                self._cluster_covariances = results['covariances']\n",
    "\n",
    "            print(f'Clustering results loaded from {filepath}')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {filepath} not found\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading clustering results: {e}\")\n",
    "\n",
    "    def construct_filepath(self, suffix='pkl', **extra_kwargs):\n",
    "        '''\n",
    "        Constructs a file path for saving or loading clustering results based on attributes.\n",
    "        Combines the base directory, attributes, and extra arguments into the file name.\n",
    "        '''\n",
    " \n",
    "        dir_path = self.base_dir\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        filename = self.construct_filename(suffix=suffix, **extra_kwargs)\n",
    "        return os.path.join(dir_path, filename)\n",
    "\n",
    "    def construct_filename(self, suffix='pkl', **extra_kwargs):\n",
    "        '''\n",
    "        Constructs a detailed filename for saving clustering results,\n",
    "        using class attributes and any extra keyword arguments passed.\n",
    "        '''\n",
    "\n",
    "        filename_kwargs = self.generate_kwargs_from_attrs()\n",
    "        filename_kwargs.update(extra_kwargs)\n",
    "\n",
    "        filename_parts = [f\"{k}={v}\" for k, v in filename_kwargs.items()]\n",
    "        filename = \"_\".join(filename_parts) + f\".{suffix}\"\n",
    "        \n",
    "        return filename\n",
    "\n",
    "    def generate_kwargs_from_attrs(self):\n",
    "        '''\n",
    "        Generate a dictionary of current class attributes and their values.\n",
    "        This can be used for constructing filenames or passing arguments.\n",
    "        '''\n",
    "        attrs = {\n",
    "            'algorithm': self.algorithm,\n",
    "            'k': self.k,\n",
    "            'n_clusters': self.n_clusters,\n",
    "            'seed': self.seed\n",
    "        }\n",
    "        \n",
    "        return attrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8674d677-2da7-4183-b350-7804bb08b1e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Clustering:\n",
    "    def __init__(self, algorithm, k, n_clusters, seed=42, base_dir='clustering'):\n",
    "        self.algorithm = algorithm  \n",
    "        self.k = k  \n",
    "        self.n_clusters = n_clusters\n",
    "        self.seed = seed\n",
    "        self.base_dir = f'../data/{base_dir}'\n",
    "\n",
    "        self._fitted_model = None\n",
    "        self._cluster_assignments = None        # cluster assignments from the model\n",
    "        self._cluster_centers = None            # cluster centers from the model\n",
    "        self._cluster_covariances = None        # cluster covariances from the model\n",
    "        self._empirical_posteriors = None       # empirical posteriors (P(g, c))\n",
    "\n",
    "    def fit(self, core_vectors, labels=None):\n",
    "        '''\n",
    "        Perform clustering on the core_vectors of a specific layer.\n",
    "        \n",
    "        Args:\n",
    "        - core_vectors (Tensor): Ex-\"peepholes\" with reduced dimension (n_samples, k)\n",
    "        - labels (Tensor): Labels for empirical posteriors computation (optional)\n",
    "        '''\n",
    "        if self.algorithm == 'gmm':\n",
    "            model = GaussianMixture(n_components=self.n_clusters, random_state=self.seed)\n",
    "            model.fit(core_vectors)\n",
    "            self._cluster_assignments = model.predict(core_vectors)\n",
    "            self._cluster_centers = model.means_\n",
    "            self._cluster_covariances = model.covariances_\n",
    "            self._fitted_model = model\n",
    "            \n",
    "        elif self.algorithm == 'kmeans':\n",
    "            model = KMeans(n_clusters=self.n_clusters, random_state=self.seed)\n",
    "            model.fit(core_vectors)\n",
    "            self._cluster_assignments = model.predict(core_vectors)\n",
    "            self._cluster_centers = model.cluster_centers_\n",
    "            self._fitted_model = model\n",
    "\n",
    "        # compute empirical posteriors if labels are provided\n",
    "        if labels is not None:\n",
    "            self.compute_empirical_posteriors(labels)\n",
    "\n",
    "    def compute_empirical_posteriors(self, labels):\n",
    "        '''\n",
    "        Compute the empirical posterior matrix P, where P(g, c) is the probability\n",
    "        that a sample assigned to cluster g belongs to class c.\n",
    "\n",
    "        Args:\n",
    "        - labels (Tensor): True class labels for the samples (n_samples, )\n",
    "        '''\n",
    "        n_samples = len(labels)\n",
    "        n_classes = len(torch.unique(labels))\n",
    "        \n",
    "        # initialize matrix to count occurrences of (cluster g, class c) pairs\n",
    "        P_counts = torch.zeros(self.n_clusters, n_classes)\n",
    "\n",
    "        # count occurrences of (cluster g, class c) pairs\n",
    "        for i in range(n_samples):\n",
    "            c = int(labels[i].item())  # true class label\n",
    "            g = int(self._cluster_assignments[i])  # cluster assignment\n",
    "            P_counts[g, c] += 1\n",
    "\n",
    "        # normalize to get empirical posteriors\n",
    "        P_empirical = P_counts / P_counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # nandle potential division by zero\n",
    "        P_empirical = torch.nan_to_num(P_empirical)  # replace NaN with 0\n",
    "\n",
    "        self._empirical_posteriors = P_empirical\n",
    "\n",
    "    def cluster_probabilities(self, core_vectors):\n",
    "        '''\n",
    "        Get cluster probabilities for the provided core_vectors based on the fitted model.\n",
    "        \n",
    "        Args:\n",
    "        - core_vectors (Tensor): Peepholes with reduced dimension (n_samples, k)\n",
    "        \n",
    "        Returns:\n",
    "        - cluster_probs (Tensor): Probabilities for each cluster (n_samples, n_clusters)\n",
    "        '''\n",
    "        if self.algorithm == 'gmm':\n",
    "            return self._fitted_model.predict_proba(core_vectors)  # (n_samples, n_clusters)\n",
    "        elif self.algorithm == 'kmeans':\n",
    "            # get distances to each cluster center\n",
    "            distances = self._fitted_model.transform(core_vectors)\n",
    "            distances = torch.tensor(distances)\n",
    "            # convert distances to probabilities (soft assignment)\n",
    "            cluster_probs = torch.exp(-distances ** 2 / (2 * (distances.std() ** 2)))  # Gaussian-like softmax\n",
    "            cluster_probs = cluster_probs / cluster_probs.sum(dim=1, keepdim=True)  # normalize to probabilities\n",
    "            return cluster_probs\n",
    "\n",
    "    def map_clusters_to_classes(self, core_vectors):\n",
    "        '''\n",
    "        Map the cluster probabilities to class probabilities using empirical posteriors.\n",
    "        \n",
    "        Args:\n",
    "        - cluster_probs (Tensor): Probabilities for each cluster (n_samples, n_clusters)\n",
    "        \n",
    "        Returns:\n",
    "        - class_probs (Tensor): Probabilities for each class (n_samples, n_classes)\n",
    "        '''\n",
    "        if self._empirical_posteriors is None:\n",
    "            raise RuntimeError('Please run compute_empirical_posteriors() first.')\n",
    "\n",
    "        cluster_probs = self.cluster_probabilities(core_vectors)\n",
    "        cluster_probs = torch.tensor(cluster_probs, dtype=torch.float32)\n",
    "        \n",
    "        class_probs = torch.matmul(cluster_probs, self._empirical_posteriors)  # shape: (n_samples, n_classes)\n",
    "        class_probs = class_probs / class_probs.sum(dim=1, keepdim=True) # aka the new peepholes\n",
    "        return class_probs\n",
    "\n",
    "    # save/load results ---------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde15673-241c-42b8-8131-731acdbeb57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'gmm'\n",
    "k = 20\n",
    "n_clusters = int(num_classes/2)\n",
    "\n",
    "clustering = Clustering(algorithm, k, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b994b1-e66a-4c9d-8677-ec07dd29d970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare data\n",
    "core_vectors = {}\n",
    "v_labels = {}\n",
    "decisions = {}\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    first_batch = next(iter(ph_dl['train']))\n",
    "    p = first_batch['peepholes']\n",
    "    layer_keys = p.keys()  \n",
    "    \n",
    "    core_vectors[split] = {key: [] for key in layer_keys}\n",
    "    v_labels[split] = []\n",
    "    decisions[split] = []\n",
    "\n",
    "    for batch in ph_dl[split]:\n",
    "        \n",
    "        peepholes = batch['peepholes']\n",
    "        labels = batch['label']\n",
    "        decision_results = batch['result']\n",
    "\n",
    "        for layer, peephole_tensor in peepholes.items():\n",
    "            batch_size, d = peephole_tensor.shape\n",
    "                            \n",
    "            reduced_peephole = peephole_tensor[:, :k]\n",
    "\n",
    "            core_vectors[split][layer].append(reduced_peephole)\n",
    "        v_labels[split].append(labels)\n",
    "        decisions[split].append(decision_results.bool())\n",
    "        \n",
    "    for layer in core_vectors[split]:\n",
    "        core_vectors[split][layer] = torch.cat(core_vectors[split][layer], dim=0) \n",
    "    \n",
    "    v_labels[split] = torch.cat(v_labels[split], dim=0)\n",
    "    decisions[split] = torch.cat(decisions[split], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae3ed5e-a779-4e34-b6a9-3980c1043aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vars(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6a183-c439-42b2-a95b-c6348b682f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit clustering model\n",
    "split = 'train'\n",
    "layer = 'classifier.0'\n",
    "labels = v_labels[split]\n",
    "\n",
    "clustering.fit(core_vectors[split][layer], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3c839b-6be3-4117-829b-9428100c6c84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(clustering._empirical_posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0fe8c1-0f58-4c89-b461-0ef8685e202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_scores = {}\n",
    "entropy_scores = {}\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    class_probs = clustering.map_clusters_to_classes(core_vectors[split][layer])\n",
    "    _max = clustering.get_confidence_scores(class_probs, split=split, score_type='max')\n",
    "    _entropy = clustering.get_confidence_scores(class_probs, split=split, score_type='entropy')\n",
    "\n",
    "    max_scores[split] = _max\n",
    "    entropy_scores[split] = _entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518c8e2-1d99-4cb5-8549-323b1a44bd2c",
   "metadata": {},
   "source": [
    "### structure without tensordict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b58782-f632-4540-81f8-bb7b2bb4d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2ff55-2916-492f-9107-b4d232e830eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_list = [20]\n",
    "n_clusters_list = [50]\n",
    "\n",
    "# select algorithm\n",
    "algorithm = 'gmm'\n",
    "\n",
    "# loop over k and n_clusters\n",
    "for k in k_list:\n",
    "    # prepare data\n",
    "    core_vectors = {}\n",
    "    v_labels = {}\n",
    "    decisions = {}\n",
    "    print('Preparing data')\n",
    "    for split in ['train', 'val']: #, 'test']:\n",
    "        first_batch = next(iter(ph_dl['train']))\n",
    "        p = first_batch['peepholes']\n",
    "        layer_keys = p.keys() # so we can automatically loop over available layers \n",
    "        \n",
    "        core_vectors[split] = {key: [] for key in layer_keys}\n",
    "        v_labels[split] = []\n",
    "        decisions[split] = []\n",
    "    \n",
    "        for batch in ph_dl[split]:\n",
    "            \n",
    "            peepholes = batch['peepholes']\n",
    "            labels = batch['label']\n",
    "            decision_results = batch['result']\n",
    "    \n",
    "            for layer, peephole_tensor in peepholes.items():\n",
    "                batch_size, d = peephole_tensor.shape\n",
    "                                \n",
    "                reduced_peephole = peephole_tensor[:, :k]\n",
    "    \n",
    "                core_vectors[split][layer].append(reduced_peephole)\n",
    "            v_labels[split].append(labels)\n",
    "            decisions[split].append(decision_results.bool())\n",
    "            \n",
    "        for layer in core_vectors[split]:\n",
    "            core_vectors[split][layer] = torch.cat(core_vectors[split][layer], dim=0) \n",
    "        \n",
    "        v_labels[split] = torch.cat(v_labels[split], dim=0)\n",
    "        decisions[split] = torch.cat(decisions[split], dim=0)\n",
    "    print('Data is ready')\n",
    "    # clustering init and fit for each layer\n",
    "    for n_clusters in tqdm(n_clusters_list):\n",
    "        split = 'train'\n",
    "        \n",
    "        clustering = {}\n",
    "        for layer in layer_keys:\n",
    "            \n",
    "            clustering[layer] = Clustering(algorithm, k, n_clusters)\n",
    "            labels = v_labels[split]\n",
    "            clustering[layer].fit(core_vectors[split][layer], labels)\n",
    "    \n",
    "        # use train split (get scores)\n",
    "        max_scores = {}\n",
    "        entropy_scores = {}\n",
    "        split = 'train'\n",
    "        class_probs = clustering[layer].map_clusters_to_classes(core_vectors[split][layer])\n",
    "        _max = clustering[layer].get_confidence_scores(class_probs, split=split, score_type='max')\n",
    "        _entropy = clustering[layer].get_confidence_scores(class_probs, split=split, score_type='entropy')\n",
    "    \n",
    "        max_scores[split] = _max\n",
    "        entropy_scores[split] = _entropy\n",
    "    \n",
    "        # use val split\n",
    "        split = 'val'\n",
    "        class_probs = clustering[layer].map_clusters_to_classes(core_vectors[split][layer])\n",
    "        _max = clustering[layer].get_confidence_scores(class_probs, split=split, score_type='max')\n",
    "        _entropy = clustering[layer].get_confidence_scores(class_probs, split=split, score_type='entropy')\n",
    "    \n",
    "        max_scores[split] = _max\n",
    "        entropy_scores[split] = _entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b44aadf-2eab-47a2-91d7-e99716dbfe33",
   "metadata": {},
   "source": [
    "### structure with tensordict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6cc92d-af13-468b-915a-e29bc9b08bb8",
   "metadata": {},
   "source": [
    "Hopefully will look something like:\n",
    "```python\n",
    "all_scores = {\n",
    "    20: {  # k value\n",
    "        50: {  # n_clusters value\n",
    "            'train': {\n",
    "                'layer1': {\n",
    "                    'max': tensor([...]),\n",
    "                    'entropy': tensor([...])\n",
    "                },\n",
    "                'layer2': {\n",
    "                    'max': tensor([...]),\n",
    "                    'entropy': tensor([...])\n",
    "                }\n",
    "            },\n",
    "            'val': {\n",
    "                'layer1': {\n",
    "                    'max': tensor([...]),\n",
    "                    'entropy': tensor([...])\n",
    "                },\n",
    "                'layer2': {\n",
    "                    'max': tensor([...]),\n",
    "                    'entropy': tensor([...])\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05bb501-6373-470e-a6b6-583b180085dd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_list = [20]\n",
    "n_clusters_list = [50]\n",
    "\n",
    "# select algorithm\n",
    "algorithm = 'gmm'\n",
    "\n",
    "# init results container\n",
    "#all_scores = TensorDict()\n",
    "all_scores = TensorDict({}, batch_size=[])\n",
    "\n",
    "# loop over k and n_clusters\n",
    "for k in k_list:\n",
    "    all_scores.set(str(k), TensorDict({}, batch_size=[]))  # set for k\n",
    "    # prepare data\n",
    "    print('Preparing data')\n",
    "    core_vectors = {}\n",
    "    v_labels = {}\n",
    "    decisions = {}\n",
    "\n",
    "    for split in ['train', 'val']: #, 'test']:\n",
    "        first_batch = next(iter(ph_dl['train']))\n",
    "        p = first_batch['peepholes']\n",
    "        layer_keys = p.keys() # so we can automatically loop over available layers \n",
    "        \n",
    "        core_vectors[split] = {key: [] for key in layer_keys}\n",
    "        v_labels[split] = []\n",
    "        decisions[split] = []\n",
    "    \n",
    "        for batch in ph_dl[split]:\n",
    "            \n",
    "            peepholes = batch['peepholes']\n",
    "            labels = batch['label']\n",
    "            decision_results = batch['result']\n",
    "    \n",
    "            for layer, peephole_tensor in peepholes.items():\n",
    "                batch_size, d = peephole_tensor.shape\n",
    "                                \n",
    "                reduced_peephole = peephole_tensor[:, :k]\n",
    "    \n",
    "                core_vectors[split][layer].append(reduced_peephole)\n",
    "            v_labels[split].append(labels)\n",
    "            decisions[split].append(decision_results.bool())\n",
    "            \n",
    "        for layer in core_vectors[split]:\n",
    "            core_vectors[split][layer] = torch.cat(core_vectors[split][layer], dim=0) \n",
    "        \n",
    "        v_labels[split] = torch.cat(v_labels[split], dim=0)\n",
    "        decisions[split] = torch.cat(decisions[split], dim=0)\n",
    "    print('Data is ready')\n",
    "    # clustering init and fit for each layer\n",
    "    for n_clusters in tqdm(n_clusters_list):\n",
    "\n",
    "        #all_scores[k] = {n_clusters: {}}\n",
    "        all_scores[str(k)].set(str(n_clusters), TensorDict({\n",
    "            'train': TensorDict({}, batch_size=[]),\n",
    "            'val': TensorDict({}, batch_size=[])\n",
    "        }, batch_size=[]))\n",
    "        \n",
    "        # fit clustering model for each layer on train split\n",
    "        clustering = {}\n",
    "        for layer in tqdm(layer_keys):\n",
    "            clustering[layer] = Clustering(algorithm, k, n_clusters)\n",
    "            labels = v_labels['train']  # Use the 'train' split labels\n",
    "            clustering[layer].fit(core_vectors['train'][layer], labels)\n",
    "\n",
    "        # compute max and entropy scores for both 'train' and 'val'\n",
    "        for split in ['train', 'val']:\n",
    "            all_scores[str(k)][str(n_clusters)][split] = {}\n",
    "            for layer in layer_keys:\n",
    "                class_probs = clustering[layer].map_clusters_to_classes(core_vectors[split][layer])\n",
    "\n",
    "                _max = clustering[layer].get_confidence_scores(class_probs, split=split, score_type='max')\n",
    "                _entropy = clustering[layer].get_confidence_scores(class_probs, split=split, score_type='entropy')\n",
    "\n",
    "                all_scores[str(k)][str(n_clusters)][split][layer] = {\n",
    "                    'max': _max,\n",
    "                    'entropy': _entropy\n",
    "                }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f3b1d-bcab-4c7b-a329-eb47e268f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores[str(k)][str(n_clusters)]['val']['classifier.0']['entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8332d0a-af03-4ae2-b196-5e0a454e1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get right filepname and save td + metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6875269b-5e7e-40fd-9c5b-332eeda312a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_path = '/srv/newpenny/XAI/generated_data/clustering'\n",
    "res_dir = 'confidence_scores'\n",
    "res_path = os.path.join(abs_path, res_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431089e-023d-4290-8d21-adbb678f4444",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = 'vgg16'\n",
    "res_suffix = '.pth'\n",
    "res_filename = f'algorithm={algorithm}_dataset={dataset}_dnn={dnn_model}'\n",
    "torch.save(all_scores, os.path.join(res_path, res_filename + res_suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc3fe99-8c48-4e8e-b02a-65a0d1276a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'k_values': k_list,\n",
    "    'n_clusters': n_clusters_list,\n",
    "    'layers': list(layer_keys)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f862976-f28c-48ba-a6ff-7fd5fb471082",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88945b54-89de-4d64-aae8-6396b20dab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86a9c0-b123-4771-ab11-7f8108e9b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clustering.clustering import prepare_data, compute_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6aa89d-6abb-47b5-b557-eae4255bf11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'classifier.0'\n",
    "k = 20\n",
    "data = prepare_data(ph_dl, [layer], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c359c-e770-447e-b23b-091cc30ab85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['core_vectors']['train'][layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ac39f-7cb7-42c0-b9e9-de00c5893707",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['true_labels']['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a248e00-46e0-4cb0-a64f-c3ab27002ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = Clustering('gmm', 20, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b62c0a-22b9-46c0-ad43-04bb469ab71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk.fit(data['core_vectors']['train'][layer], data['true_labels']['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a16615-5e7f-479a-89e4-ef5d3d1863f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk.compute_empirical_posteriors(data['true_labels']['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c157e4-ec95-4c21-b233-21ce0b5a2257",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_vectors = data['core_vectors']\n",
    "v_labels = data['true_labels']\n",
    "decisions = data['decisions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84857e-a5b5-41e1-8e29-317c725e9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_layers = list(next(iter(ph_dl['train']))['peepholes'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7dffc-7b8c-484f-adbf-a7e4f65d3abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_list = available_layers\n",
    "compute_scores(k, n_clusters, layers_list, data, all_scores, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3e004-1f9d-485b-8260-d3d002c653e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['core_vectors'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0986b0-eb7f-45a0-9ccc-f6f8e658c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for split in data['core_vectors'].keys():\n",
    "for layer in [layer]:\n",
    "    compute_scores(20, 50, 'gmm', [layer], data, all_scores=None, metadata=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f2cdc6-b4ad-489a-8227-dc89f4c84b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_keys = layers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cec59c-aae2-4294-b040-24f574bd6104",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# init params\n",
    "k_list = [20]\n",
    "n_clusters_list = [50, 100]\n",
    "algorithm = 'gmm'\n",
    "\n",
    "# check for existing results or init results container\n",
    "res_suffix = '.pth'\n",
    "res_filename = f'algorithm={algorithm}_dataset={dataset}_dnn={dnn_model}'\n",
    "tensor_dict_path = os.path.join(res_path, res_filename + res_suffix)\n",
    "\n",
    "meta_filename = '_'.join(['metadata', res_filename]) + '.json'\n",
    "meta_path = os.path.join(res_path, meta_filename)\n",
    "\n",
    "# check if results and metadata exist\n",
    "results_exist = os.path.exists(tensor_dict_path)\n",
    "metadata_exist = os.path.exists(meta_path)\n",
    "\n",
    "if results_exist:\n",
    "    print('Results already present')\n",
    "    all_scores = torch.load(tensor_dict_path)\n",
    "else:\n",
    "    all_scores = TensorDict({}, batch_size=[])\n",
    "\n",
    "if metadata_exist:\n",
    "    print('Loading related metadata')\n",
    "    with open(meta_path, 'r') as json_file:\n",
    "        metadata = json.load(json_file)\n",
    "else:\n",
    "    metadata = {'k_values': [], 'n_clusters': [], 'layers': []}\n",
    "\n",
    "# loop over k and n_clusters\n",
    "for k in k_list:\n",
    "    str_k = str(k) if not isinstance(k, str) else k\n",
    "\n",
    "    if str_k not in all_scores.keys():\n",
    "        all_scores.set(str_k, TensorDict({}, batch_size=[]))\n",
    "\n",
    "    for n_clusters in n_clusters_list:\n",
    "        str_n_clusters = str(n_clusters) if not isinstance(n_clusters, str) else n_clusters\n",
    "\n",
    "        # check if the combination of k and n_clusters already exists in the scores\n",
    "        if str_n_clusters in all_scores[str_k].keys():\n",
    "            existing_layers = all_scores[str_k][str_n_clusters]['train'].keys()\n",
    "\n",
    "            # check if only specific layers need to be computed\n",
    "            if existing_layers and any(layer not in existing_layers for layer in layer_keys):\n",
    "                # compute only the missing layers\n",
    "                for layer in layer_keys:\n",
    "                    if layer not in existing_layers:\n",
    "                        data = prepare_data(ph_dl, [layer], k)\n",
    "                        compute_scores(k, n_clusters, [layer], data, all_scores, metadata)\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping k={k}, n_clusters={n_clusters} for all layers as it's already computed.\")\n",
    "                continue  # skip to the next n_clusters if all layers have data\n",
    "\n",
    "        else:\n",
    "            # if not existing, create the subdict for n_clusters and compute all layers\n",
    "            all_scores[str_k].set(str_n_clusters, TensorDict({\n",
    "                'train': TensorDict({}, batch_size=[]),\n",
    "                'val': TensorDict({}, batch_size=[])\n",
    "            }, batch_size=[]))\n",
    "            existing_layers = []  # no existing layers yet\n",
    "\n",
    "            # prepare data for all splits\n",
    "            data = prepare_data(ph_dl, layer_keys, k)\n",
    "\n",
    "            # compute scores for all layers and splits\n",
    "            for split in data['core_vectors'].keys():\n",
    "                for layer in layer_keys:\n",
    "                    compute_scores(k, n_clusters, layer, all_scores, metadata)\n",
    "\n",
    "# Save all scores and metadata after processing\n",
    "torch.save(all_scores, tensor_dict_path)\n",
    "with open(meta_path, 'w') as json_file:\n",
    "    json.dump(metadata, json_file, indent=4)\n",
    "\n",
    "print(\"Results and metadata saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c3e31-a9bd-4a77-8c11-fa943f9ca1f2",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "* split val data into correct/wrong\n",
    "* make confusion matrix for each threshold on the training scores\n",
    "* do it for single layers and combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c30ef-551c-4831-8d94-1143091daa17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1dbf1-05a3-4014-9158-2d62caee4e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "# if you need only labels and decision outcomes, k and layers_list are irrelevant\n",
    "data = prepare_data(ph_dl, available_layers[:1], k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed671332-74cb-4395-bdf2-52bc19e5616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correct/wrong decision indices\n",
    "decisions = data['decisions']\n",
    "decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c40749b-c50f-4491-8b8d-b60a7026754c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load collected scores\n",
    "dataset = 'CIFAR100'\n",
    "dnn_model = 'vgg16'\n",
    "algorithm = 'gmm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a0a99d-1263-4869-a40d-3dd32403181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load existing results\n",
    "# check for existing results or init results container\n",
    "res_dir = 'clustering/confidence_scores'\n",
    "res_path = os.path.join(abs_path, res_dir)\n",
    "tensor_dict_path = os.path.join(res_path, f'algorithm={algorithm}_dataset={dataset}_dnn={dnn_model}.memmap')\n",
    "\n",
    "if os.path.exists(tensor_dict_path):\n",
    "    print('Results already present')\n",
    "    all_scores = TensorDict.load_memmap(tensor_dict_path)\n",
    "else:\n",
    "    all_scores = TensorDict({}, batch_size=[])\n",
    "\n",
    "# init the peephole container if not existing\n",
    "new_peep_dir = 'clustering/peepholes' \n",
    "new_peep_path = os.path.join(abs_path, new_peep_dir) \n",
    "new_peep_tensor_dict_path = os.path.join(new_peep_path, f'algorithm={algorithm}_dataset={dataset}_dnn={dnn_model}.memmap')\n",
    "\n",
    "if os.path.exists(new_peep_tensor_dict_path):\n",
    "    print('New peepholes results already present')\n",
    "    peephole_scores = TensorDict.load_memmap(new_peep_tensor_dict_path)\n",
    "else:\n",
    "    # print('Initializing peephole container')\n",
    "    peephole_scores = TensorDict({}, batch_size=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a1a48-51e1-4d4a-adf5-38f12f16d960",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values_list, n_clusters_list, splits_list, layers_list = get_unique_values(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a21f3b-ecc8-4bb5-b079-08fea6a83c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90596a19-77bd-4801-9481-b9e8b04c89f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb1bf72-dfe0-44c3-8574-8ec63e4be4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc8360-ae21-4e90-a592-a7d9b491066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4700ded5-c214-4963-a987-270a3ae6f539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824cb331-64c0-4dfa-9541-040263e44283",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_values_list:\n",
    "    for n in n_clusters_list:\n",
    "        for layer in layers_list:\n",
    "            for measure in ['max', 'entropy']:\n",
    "                list_correct = [] \n",
    "                list_wrong = []  \n",
    " \n",
    "                conf_t = all_scores[k][n]['train'][layer][measure] \n",
    "                conf_v = all_scores[k][n]['val'][layer][measure]    \n",
    "            \n",
    "                for i in quantiles:\n",
    "                    # compute threshold\n",
    "                    q = torch.quantile(conf_t, i)\n",
    "\n",
    "                    if measure=='max':\n",
    "                        # get indices where validation scores exceed the threshold\n",
    "                        idx = torch.where(conf_v > q)[0]  \n",
    "                    elif measure=='entropy':\n",
    "                        torch.where(conf_v < q)[0]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a0f6b-76ac-4dd3-bdc4-69544418a0e2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for j, (layer, weight) in enumerate(w_dict.items()):\n",
    "\n",
    "    for n in num_clusters:    \n",
    "        \n",
    "        fig, axs = plt.subplots(1, figsize=(6, 6))\n",
    "        axs.grid()\n",
    "        axs.plot([0, 1],[1,0],label='ref', c='k', ls='--')\n",
    "        \n",
    "        for dim in dims_list: \n",
    "            \n",
    "            prob_train = out_p_prob_train[layer][(dim, n)]\n",
    "            prob_val = out_p_prob_val[layer][(dim, n)]\n",
    "            \n",
    "            if measure=='max':\n",
    "                conf_t = np.max(prob_train,axis=1)\n",
    "                conf_v = np.max(prob_val,axis=1)\n",
    "            elif measure=='entropy':\n",
    "                conf_t = H(prob_train,axis=1)\n",
    "                conf_v = H(prob_val,axis=1)\n",
    "            \n",
    "            threshold = []\n",
    "            list_true_max_ = []\n",
    "            list_false_max_ = []\n",
    "            \n",
    "            for i in array:\n",
    "            \n",
    "                perc = np.quantile(conf_t, i)\n",
    "                \n",
    "                threshold.append(perc)\n",
    "                idx = np.argwhere(conf_v>perc)[:,0]\n",
    "                counter = collections.Counter(results_v[idx])\n",
    "                list_true_max_.append(counter[True]/tot_true_v)\n",
    "                list_false_max_.append(counter[False]/tot_false_v)  \n",
    "\n",
    "            axs.plot(array, list_true_max_, alpha=0.5)\n",
    "            axs.plot(array, list_false_max_, label=f'{dim}', alpha=0.5)\n",
    "            \n",
    "            axs.legend()\n",
    "            fig.suptitle(f'RF with {measure} n_clusters={n} layer={layer}\\n', fontsize=12)\n",
    "            # axs[j].set_title(f'weights={formatted_weight}')\n",
    "            #axs[j,k].title(f'dim={dim} num_clusters={n}', fontsize=16)\n",
    "            \n",
    "            \n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f106b0c-5dea-4894-8e9a-ea2c48309cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8bae67-bb31-4d4d-ba65-bed913d7e2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0488a3e-47e5-4eb2-aa26-4da2f50e57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val, idx = torch.topk(cs, 5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656369a9-938c-4867-a569-3649d7994462",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f787ae-4218-4ca7-bf6b-dda773bbabbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(v_labels['train']).to(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dca618-0401-48e9-8991-0c53143e18dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum((v_labels['train']).to(int)==torch.argmax(cs, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589e34c-e7cd-4b76-870c-beada4280bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum((v_labels['train']).to(int)==idx[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7f0fc-446d-42f3-a994-3b20a43fc0de",
   "metadata": {},
   "source": [
    "## Testing with memmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd7b59-af10-46d3-81f2-6d23697a52ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from tensordict import TensorDict\n",
    "from tensordict import MemoryMappedTensor as MMT\n",
    "from datasets.cifar import Cifar\n",
    "from models.model_wrap import ModelWrap \n",
    "from peepholes.peepholes import Peepholes\n",
    "from clustering.clustering import Clustering\n",
    "from clustering.clustering import prepare_data, compute_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49992efb-acba-4d34-8a76-ef5e3c157589",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "cuda_index = torch.cuda.device_count() - 2\n",
    "device = torch.device(f\"cuda:{cuda_index}\" if use_cuda else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "#--------------------------------\n",
    "# Parameters\n",
    "#--------------------------------\n",
    "dnn_model = 'vgg16'\n",
    "abs_path = '/srv/newpenny/XAI/generated_data'\n",
    "dataset = 'CIFAR100' \n",
    "seed = 29\n",
    "bs = 64\n",
    "ds = Cifar(dataset=dataset)\n",
    "\n",
    "ds.load_data(batch_size=bs, data_kwargs={'num_workers': 4, 'pin_memory': True}, seed=seed) \n",
    "print('Loading the ex-peepholes')\n",
    "\n",
    "phs_name = 'peepholes'\n",
    "phs_dir = os.path.join(abs_path, 'peepholes')\n",
    "peepholes = Peepholes(path=phs_dir, name=phs_name)\n",
    "loaders = ds.get_dataset_loaders()\n",
    "\n",
    "# Copy dataset to peepholes dataset\n",
    "peepholes.get_peep_dataset(loaders=loaders, verbose=True) \n",
    "ph_dl = peepholes.get_dataloaders(batch_size=128, verbose=True)\n",
    "\n",
    "available_layers = list(next(iter(ph_dl['train']))['peepholes'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6552364e-33ad-45c4-8646-f3cb01d0772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing confidence scores')\n",
    "\n",
    "k_list = [20, 50, 70]\n",
    "n_clusters_list = [50, 100, 150, 200]\n",
    "layers_list = available_layers\n",
    "algorithm = 'gmm'\n",
    "\n",
    "# check for existing results or init results container\n",
    "res_dir = 'clustering/confidence_scores'\n",
    "res_path = os.path.join(abs_path, res_dir)\n",
    "tensor_dict_path = os.path.join(res_path, f'algorithm={algorithm}_dataset={dataset}_dnn={dnn_model}.memmap')\n",
    "\n",
    "if os.path.exists(tensor_dict_path):\n",
    "    print('Results already present')\n",
    "    all_scores = TensorDict.load_memmap(tensor_dict_path)\n",
    "else:\n",
    "    all_scores = TensorDict({}, batch_size=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa91709-62f1-4d8a-8650-daed0f56bc0b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# converting my old data to memmap\n",
    "\n",
    "# Ensure the results directory exists\n",
    "res_dir = 'clustering/confidence_scores'\n",
    "res_path = os.path.join('/srv/newpenny/XAI/generated_data', res_dir)\n",
    "\n",
    "if not os.path.exists(res_path):\n",
    "    os.makedirs(res_path)\n",
    "\n",
    "# Loading existing scores from .pth file if needed\n",
    "old_tensor_dict_path = os.path.join(res_path, 'algorithm=gmm_dataset=CIFAR100_dnn=vgg16.pth')\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(old_tensor_dict_path):\n",
    "    # Load existing all_scores\n",
    "    all_scores = torch.load(old_tensor_dict_path)\n",
    "\n",
    "    # Create a new MemoryMappedTensor with the same structure\n",
    "    new_all_scores = TensorDict({}, batch_size=[])\n",
    "\n",
    "    for k in all_scores.keys():\n",
    "        new_all_scores.set(str(k), TensorDict({}, batch_size=[]))\n",
    "        for n_clusters in all_scores[str(k)].keys():\n",
    "            new_all_scores[str(k)].set(str(n_clusters), TensorDict({}, batch_size=[]))\n",
    "            for split in all_scores[str(k)][str(n_clusters)].keys():\n",
    "                new_all_scores[str(k)][str(n_clusters)].set(split, TensorDict({}, batch_size=[]))\n",
    "                for layer in all_scores[str(k)][str(n_clusters)][split].keys():\n",
    "                    # Create MemoryMappedTensor for max and entropy\n",
    "                    _max = all_scores[str(k)][str(n_clusters)][split][layer]['max']\n",
    "                    _entropy = all_scores[str(k)][str(n_clusters)][split][layer]['entropy']\n",
    "\n",
    "                    new_all_scores[str(k)][str(n_clusters)][split][layer] = {\n",
    "                        'max': MMT(_max.shape),\n",
    "                        'entropy': MMT(_entropy.shape)\n",
    "                    }\n",
    "                    # Populate the MMT with existing data\n",
    "                    new_all_scores[str(k)][str(n_clusters)][split][layer]['max'].copy_(_max)\n",
    "                    new_all_scores[str(k)][str(n_clusters)][split][layer]['entropy'].copy_(_entropy)\n",
    "\n",
    "    # Save the new MemoryMappedTensor\n",
    "    new_tensor_dict_path = os.path.join(res_path, 'algorithm=gmm_dataset=CIFAR100_dnn=vgg16.memmap')\n",
    "    new_all_scores.memmap(new_tensor_dict_path, num_threads=4)\n",
    "\n",
    "    print(f\"Converted and saved new all_scores with memory mapping at {new_tensor_dict_path}.\")\n",
    "else:\n",
    "    print(f\"No existing scores found at {old_tensor_dict_path}. Please check the path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f3f82-0fb5-45f5-839b-36d30d065227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the peephole container if not existing\n",
    "new_peep_dir = 'clustering/peepholes' \n",
    "new_peep_path = os.path.join(abs_path, new_peep_dir) \n",
    "new_peep_tensor_dict_path = os.path.join(new_peep_path, f'algorithm={algorithm}_dataset={dataset}_dnn={dnn_model}.memmap')\n",
    "\n",
    "if os.path.exists(new_peep_tensor_dict_path):\n",
    "    print('New peepholes results already present')\n",
    "    peephole_scores = TensorDict.load_memmap(new_peep_tensor_dict_path)\n",
    "else:\n",
    "    print('Initializing peephole container')\n",
    "    peephole_scores = TensorDict({}, batch_size=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee66ee-330d-4644-b6fc-67c2ba2127b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peephole_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b26ceaf-b077-47bd-a815-d56e9b1c5d86",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Loop over k and n_clusters\n",
    "for k in k_list:\n",
    "    str_k = str(k)\n",
    "    if str_k not in all_scores.keys():\n",
    "        all_scores.set(str_k, TensorDict({}, batch_size=[]))\n",
    "    \n",
    "    for n_clusters in n_clusters_list:\n",
    "        str_n_clusters = str(n_clusters)\n",
    "\n",
    "        # Check if the combination of k and n_clusters already exists in the scores\n",
    "        if str_n_clusters in all_scores[str_k].keys():\n",
    "            existing_layers = all_scores[str_k][str_n_clusters]['train'].keys()\n",
    "            \n",
    "            # Compute only the missing layers\n",
    "            for layer in layers_list:\n",
    "                if layer not in existing_layers:\n",
    "                    data = prepare_data(ph_dl, [layer], k)\n",
    "                    print(f'Clustering for layer={layer}')\n",
    "                    compute_scores(k, n_clusters, algorithm, [layer], data, all_scores, metadata)\n",
    "        else:\n",
    "            print(f'Clustering with algorithm={algorithm}, k={k}, n_clusters={n_clusters}')\n",
    "            # If not existing, create the subdict for n_clusters\n",
    "            all_scores[str_k].set(str_n_clusters, TensorDict({\n",
    "                'train': TensorDict({}, batch_size=[]),\n",
    "                'val': TensorDict({}, batch_size=[])\n",
    "            }, batch_size=[]))\n",
    "            \n",
    "            # Prepare data for all splits\n",
    "            data = prepare_data(ph_dl, layers_list, k)\n",
    "            compute_scores(k, n_clusters, algorithm, layers_list, data, all_scores, metadata)\n",
    "\n",
    "# Pre-allocate memory-mapped tensors for scores\n",
    "for layer in layers_list:\n",
    "    n_samples = len(loaders['train'].dataset)  # Get the number of samples from the training loader\n",
    "    all_scores[str_k][str_n_clusters]['train'][layer] = MMT.empty(shape=torch.Size((n_samples,)))  # Initialize with MMT\n",
    "\n",
    "# Save all scores and metadata after processing\n",
    "all_scores.memmap(tensor_dict_path, num_threads=4)  # Specify the number of threads for saving\n",
    "with open(meta_path, 'w') as json_file:\n",
    "    json.dump(metadata, json_file, indent=4)\n",
    "\n",
    "print('Results and metadata saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f7a71-8121-4012-876a-f46477786559",
   "metadata": {},
   "outputs": [],
   "source": [
    "peephole_scores[str(k)][str(n_clusters)][split]['features.24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29550107-2351-4903-838c-c3b8130fbc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(peephole_scores[str(k)][str(n_clusters)][split]['classifier.3'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82848b66-bd38-4134-9305-ee07cfab9e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['true_labels']['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c79b9-9fc6-4a12-86c8-38a3caf35484",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = data['true_labels']['val']\n",
    "_, g = torch.max(peephole_scores[str(50)][str(150)][split]['classifier.0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdea711-b653-419e-818b-bdb3f13fd3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(c == g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fefe57-4220-4f3e-ba7d-1ed3a5f556b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66a77f7-50fa-45d1-8770-8a64cb519aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_list:  # loop over core-vector dimension\n",
    "    str_k = str(k)\n",
    "\n",
    "    # initialize peephole_scores for k\n",
    "    if str_k not in peephole_scores.keys():\n",
    "        peephole_scores.set(str_k, TensorDict({}, batch_size=[]))\n",
    "\n",
    "    for n_clusters in n_clusters_list:  # loop over n_clusters\n",
    "        str_n_clusters = str(n_clusters)\n",
    "\n",
    "        # initialize peephole_scores for n_clusters\n",
    "        if str_n_clusters not in peephole_scores[str_k].keys():\n",
    "            peephole_scores[str_k].set(str_n_clusters, TensorDict({\n",
    "                'train': TensorDict({}, batch_size=[]),\n",
    "                'val': TensorDict({}, batch_size=[]),\n",
    "            }, batch_size=[]))\n",
    "\n",
    "        # for both train and val splits, ensure layers exist\n",
    "        for split in ['train', 'val']:\n",
    "            if split not in peephole_scores[str_k][str_n_clusters].keys():\n",
    "                peephole_scores[str_k][str_n_clusters].set(split, TensorDict({}, batch_size=[]))\n",
    "\n",
    "            existing_layers = peephole_scores[str_k][str_n_clusters][split].keys()\n",
    "            \n",
    "            # check if all layers are present in the current split\n",
    "            if existing_layers and any(layer not in existing_layers for layer in layers_list):\n",
    "                for layer in layers_list:\n",
    "                    if layer not in existing_layers:\n",
    "                        data = prepare_data(ph_dl, [layer], k)\n",
    "                        print(f'Clustering for layer={layer} with n_clusters={n_clusters}')\n",
    "                        compute_scores(k, \n",
    "                                       n_clusters, \n",
    "                                       algorithm, \n",
    "                                       [layer], \n",
    "                                       data, \n",
    "                                       peephole_scores, \n",
    "                                       all_scores, \n",
    "                                       compute_scores=True, \n",
    "                                       seed=42)\n",
    "\n",
    "            else:\n",
    "                print(f'Skipping {algorithm} k={k}, n_clusters={n_clusters} for {split} layers')\n",
    "                continue  # skip to the next n_clusters if all layers have data\n",
    "\n",
    "        # if not all layers have data, we still need to compute scores\n",
    "        if str_n_clusters not in peephole_scores[str_k].keys() or not existing_layers:\n",
    "            print('Clustering')\n",
    "            print(f'algorithm={algorithm}, k={k}, n_clusters={n_clusters}')\n",
    "\n",
    "            data = prepare_data(ph_dl, layers_list, k)\n",
    "\n",
    "            n_samples_train = len(data['core_vectors']['train'][layers_list[0]])\n",
    "            n_samples_val = len(data['core_vectors']['val'][layers_list[0]])\n",
    "\n",
    "            # initialize all_scores for n_clusters\n",
    "            all_scores[str_k].set(str_n_clusters, TensorDict({\n",
    "                'train': TensorDict({\n",
    "                    layer: MMT.empty(shape=(n_samples_train,)) for layer in layers_list}, batch_size=[]),\n",
    "                'val': TensorDict({\n",
    "                    layer: MMT.empty(shape=(n_samples_val,)) for layer in layers_list}, batch_size=[])\n",
    "            }, batch_size=[]))\n",
    "\n",
    "            compute_scores(k, \n",
    "                           n_clusters, \n",
    "                           algorithm, \n",
    "                           layers_list, \n",
    "                           data, \n",
    "                           peephole_scores, \n",
    "                           all_scores, \n",
    "                           compute_scores=True, \n",
    "                           seed=42)\n",
    "\n",
    "# save results\n",
    "peephole_scores.memmap(new_peep_tensor_dict_path, num_threads=4)\n",
    "all_scores.memmap(tensor_dict_path, num_threads=4)\n",
    "print('Results saved to memory-mapped tensor.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe0dd84-1e44-45a6-91b1-a3265ac1216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dict = TensorDict({layer: MMT.empty(shape=(n_samples, n_classes)) for layer in layers_list}, batch_size=[])\n",
    "\n",
    "peephole_scores[str(k)].set(str(n_clusters), TensorDict({\n",
    "    split: layer_dict  # Pre-allocate memory-mapped tensor for each layer\n",
    "}, batch_size=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f5937-5cbb-4a0e-8310-4788ecec0ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044509d-5c55-459c-af0f-9e239148aaf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "012eb927-2139-447a-ad23-7be0a3896cc8",
   "metadata": {},
   "source": [
    "## stuff with coreVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987645d-aced-4003-b3dc-3342d0981dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6, 7\"\n",
    "os.environ['SCIPY_USE_PROPACK'] = \"True\"\n",
    " \n",
    "threads = \"64\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = threads\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = threads\n",
    "os.environ[\"MKL_NUM_THREADS\"] = threads\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = threads\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b61d027-c5a8-4a99-9df3-23eb1a34302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python stuff\n",
    "from pathlib import Path as Path\n",
    "from numpy.random import randint\n",
    "\n",
    "# Our stuff\n",
    "from datasets.cifar import Cifar\n",
    "from models.model_wrap import ModelWrap \n",
    "from coreVectors.coreVectors import CoreVectors \n",
    "from coreVectors.svd_coreVectors import reduct_matrices_from_svds as parser_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f530000-f67a-4cb5-8a9a-4dd7fe9e0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch stuff\n",
    "import torch\n",
    "from torchvision.models import vgg16, VGG16_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139fdcb1-de53-4ed4-8a58-4d25f859f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier.classifier_base import trim_corevectors\n",
    "from classifier.kmeans import KMeans \n",
    "from classifier.gmm import GMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c1d38-0b7e-4f6f-8d45-6372aba3e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf02c8-397c-4244-b886-fd6144e7a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peepholes.peepholes import Peepholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89981d8b-6611-4167-b2fd-624e21739304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_kmeans(layer_list, n_clusters_list, peep_size_list, n_classes, ph_dl, true_labels, return_preds=False):\n",
    "    results = [] \n",
    "    predictions = []\n",
    "    \n",
    "    for peep_size in peep_size_list:\n",
    "        for n_clusters in n_clusters_list:\n",
    "            print(f'KMeans')\n",
    "        \n",
    "            for layer in layer_list:\n",
    "                random_seed = np.random.randint(0, 2**32-1)\n",
    "                print(f'Layer: {layer}, Peep size: {peep_size}, seed={random_seed}')\n",
    "                \n",
    "                parser_kwargs = {'layer': layer, 'peep_size': peep_size}\n",
    "                cls_kwargs = {'random_state': random_seed, 'n_init': n_classes, 'max_iter': 500}\n",
    "                \n",
    "           \n",
    "                cls = KMeans(\n",
    "                    nl_classifier=n_clusters,\n",
    "                    nl_model=n_classes,\n",
    "                    parser=parser_cv,\n",
    "                    parser_kwargs=parser_kwargs,\n",
    "                    cls_kwargs=cls_kwargs\n",
    "                )\n",
    "                \n",
    "                cls.fit(dataloader=ph_dl['train'], verbose=False)\n",
    "                cls.compute_empirical_posteriors(verbose=False)\n",
    "                \n",
    "                #plt.imshow(cls._empp)\n",
    "                \n",
    "                peepholes = Peepholes(classifier=cls)\n",
    "                peepholes.get_peepholes(dataloader=ph_dl['val'], verbose=False)\n",
    "                preds = peepholes._phs\n",
    "                \n",
    "                acc = torch.sum(torch.argmax(preds, axis=1) == true_labels['val']).item() / len(true_labels['val'])\n",
    "                print(f'Layer: {layer}, Peep size: {peep_size}, Clusters: {n_clusters} Accuracy: {acc}')\n",
    "                \n",
    "                results.append({'layer': layer, 'n_clusters': n_clusters, 'peep_size': peep_size, 'accuracy': acc})\n",
    "                predictions.append({'layer': layer, 'n_clusters': n_clusters, 'peep_size': peep_size, \n",
    "                                    'labels_pred': torch.argmax(preds, axis=1)})\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.set_index(['layer', 'n_clusters', 'peep_size'], inplace=True)\n",
    "\n",
    "    df_pred = pd.DataFrame(predictions)\n",
    "    df_pred.set_index(['layer', 'n_clusters', 'peep_size'], inplace=True)\n",
    "\n",
    "    if return_preds:\n",
    "        return df, df_pred\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5950ed0-eec7-49e7-8625-bb65548488e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gmm(layer_list, n_clusters_list, peep_size_list, n_classes, ph_dl, true_labels, return_preds=False):\n",
    "    results = [] \n",
    "    predictions = []\n",
    "    \n",
    "    for peep_size in peep_size_list:\n",
    "        for n_clusters in n_clusters_list:\n",
    "            print(f'GMM')\n",
    "        \n",
    "            for layer in layer_list:\n",
    "                random_seed = np.random.randint(0, 2**32-1)\n",
    "                print(f'Layer: {layer}, Peep size: {peep_size}, seed={random_seed}')\n",
    "                \n",
    "                parser_kwargs = {'layer': layer, 'peep_size': peep_size}\n",
    "                cls_kwargs = {'random_state': random_seed, 'n_init': n_classes, 'max_iter': 500}\n",
    "               \n",
    "           \n",
    "                cls = GMM(\n",
    "                    nl_classifier=n_clusters,\n",
    "                    nl_model=n_classes,\n",
    "                    parser=parser_cv,\n",
    "                    parser_kwargs=parser_kwargs,\n",
    "                    cls_kwargs=cls_kwargs\n",
    "                )\n",
    "                \n",
    "                cls.fit(dataloader=ph_dl['train'], verbose=False)\n",
    "                cls.compute_empirical_posteriors(verbose=False)\n",
    "                \n",
    "                #plt.imshow(cls._empp)\n",
    "                \n",
    "                peepholes = Peepholes(classifier=cls)\n",
    "                peepholes.get_peepholes(dataloader=ph_dl['val'], verbose=False)\n",
    "                preds = peepholes._phs\n",
    "                \n",
    "                acc = torch.sum(torch.argmax(preds, axis=1) == true_labels['val']).item() / len(true_labels['val'])\n",
    "                print(f'Layer: {layer}, Peep size: {peep_size}, Clusters: {n_clusters} Accuracy: {acc}')\n",
    "                \n",
    "                results.append({'layer': layer, 'n_clusters': n_clusters, 'peep_size': peep_size, 'accuracy': acc})\n",
    "                predictions.append({'layer': layer, 'n_clusters': n_clusters, 'peep_size': peep_size, \n",
    "                                    'labels_pred': torch.argmax(preds, axis=1)})\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.set_index(['layer', 'n_clusters', 'peep_size'], inplace=True)\n",
    "\n",
    "    df_pred = pd.DataFrame(predictions)\n",
    "    df_pred.set_index(['layer', 'n_clusters', 'peep_size'], inplace=True)\n",
    "\n",
    "    if return_preds:\n",
    "        return df, df_pred\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990dd4d-4f44-46de-a74b-1edca60573df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "cuda_index = torch.cuda.device_count() - 2\n",
    "device = torch.device(f\"cuda:{cuda_index}\" if use_cuda else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2c6af-47e8-4c1d-a9f1-81a68b58ca5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# Dataset \n",
    "#--------------------------------\n",
    "# model parameters\n",
    "dataset = 'CIFAR100' \n",
    "seed = 29\n",
    "bs = 64\n",
    "\n",
    "ds_path = f'/srv/newpenny/dataset/{dataset}'\n",
    "ds = Cifar(data_path=ds_path,\n",
    "           dataset=dataset)\n",
    "ds.load_data(\n",
    "        batch_size = bs,\n",
    "        data_kwargs = {'num_workers': 4, 'pin_memory': True},\n",
    "        seed = seed,\n",
    "        )\n",
    "\n",
    "#--------------------------------\n",
    "# Model \n",
    "#--------------------------------\n",
    "pretrained = True\n",
    "model_dir = '/srv/newpenny/XAI/models'\n",
    "if dataset=='CIFAR100':\n",
    "    # CIFAR100\n",
    "    model_name = f'vgg16_pretrained={pretrained}_dataset={dataset}-'\\\n",
    "                 f'augmented_policy=CIFAR10_bs={bs}_seed={seed}.pth'\n",
    "elif dataset=='CIFAR10':\n",
    "    # CIFAR10\n",
    "    model_name = f'_vgg16_pretrained={pretrained}_dataset={dataset}-'\\\n",
    "                 f'augmented_policy=CIFAR10_seed={seed}.pth'\n",
    "\n",
    "nn = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "in_features = 4096\n",
    "num_classes = len(ds.get_classes())\n",
    "nn.classifier[-1] = torch.nn.Linear(in_features, num_classes)\n",
    "model = ModelWrap(device=device)\n",
    "model.set_model(model=nn, path=model_dir, name=model_name, verbose=True)\n",
    "\n",
    "# layers_dict = {'classifier': [0, 3],\n",
    "#                'features': [24, 28]}\n",
    "layers_dict = {'classifier': [0, 3],\n",
    "               'features': [14, 24, 28]}\n",
    "\n",
    "model.set_target_layers(target_layers=layers_dict, verbose=True)\n",
    "print('target layers: ', model.get_target_layers()) \n",
    "\n",
    "\n",
    "#--------------------------------\n",
    "# CoreVectors \n",
    "#--------------------------------\n",
    "phs_name = 'corevectors'\n",
    "phs_dir = f'/srv/newpenny/XAI/generated_data/corevectors/{dataset}'\n",
    "# phs_dir = Path.cwd()/'../data/corevectors'\n",
    "cv = CoreVectors(\n",
    "        path = phs_dir,\n",
    "        name = phs_name,\n",
    "        )\n",
    "\n",
    "loaders = ds.get_dataset_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2321bf8b-59ce-4c8c-be64-0adb44602dee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# copy dataset to coreVect dataset\n",
    "cv.get_coreVec_dataset(\n",
    "        loaders = loaders,\n",
    "        verbose = True\n",
    "        ) \n",
    "\n",
    "# cv.get_activations(\n",
    "#         model=model,\n",
    "#         loaders=loaders,\n",
    "#         verbose=True\n",
    "#         )\n",
    "\n",
    "# cv.get_coreVectors(\n",
    "#         model = model,\n",
    "#         reduct_matrices = model._svds,\n",
    "#         parser = parser_fn,\n",
    "#         verbose = True\n",
    "#         )\n",
    "\n",
    "ph_dl = cv.get_dataloaders(batch_size=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64226e07-e1ad-4456-87a5-306e85ec2f34",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e762559d-3882-42b7-b66c-92c31b04fb13",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb55e4-476a-4582-8fd4-7ab715fc25b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10 if dataset=='CIFAR10' else 100\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f39b02-f616-40db-ac0d-a769b93d1594",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'classifier.0'\n",
    "n_clusters = 150\n",
    "peep_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc5293-5772-4b06-8e9c-1de57df5bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_cv = trim_corevectors\n",
    "parser_kwargs = {'layer': layer, 'peep_size':peep_size}\n",
    "cls_kwargs = {'random_state': 42, 'n_init':n_classes, 'max_iter':500} \n",
    "\n",
    "cls = KMeans(\n",
    "        nl_classifier = n_clusters,\n",
    "        nl_model = n_classes,\n",
    "        parser = parser_cv,\n",
    "        parser_kwargs = parser_kwargs,\n",
    "        cls_kwargs = cls_kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdddba8-0cd2-445d-9f23-abf217d82dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.fit(dataloader = ph_dl['val'], verbose=True)\n",
    "cls.compute_empirical_posteriors(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097cee3a-1853-44c3-bba3-e870df9b40bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = cls.classifier_probabilities(dataloader=ph_dl['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12649d-5492-49bc-a132-466556292b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls._classifier_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886c3dfe-c71b-4404-bf00-b72516240576",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_dl = ph_dl['val']\n",
    "n_samples = len(_dl.dataset)\n",
    "bs = _dl.batch_size\n",
    "\n",
    "# Allocate predict probs\n",
    "_classifier_probs = torch.zeros(n_samples, 100)\n",
    "\n",
    "for bn, batch in enumerate(tqdm(_dl)):\n",
    "    data = cls.parser(data = batch, **cls.parser_kwargs)\n",
    "    n_in = len(data)\n",
    "    \n",
    "    distances = torch.tensor(cls._classifier.transform(data))\n",
    "\n",
    "    # convert distances to probabilities (soft assignment) Gaussian-like softmax\n",
    "    # TODO: Check the var in the exponent. Should it be over the training set? Should it be there?\n",
    "    #probs = torch.exp(-distances ** 2 / (2 * (distances.std() ** 2)))\n",
    "    probs = torch.exp(-distances ** 2 / 2 )\n",
    "\n",
    "    # normalize to probabilities\n",
    "    probs /= probs.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    _classifier_probs[bn*bs:bn*bs+n_in] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b359e216-2852-4c17-b017-61f38ef007a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c67e1e4-c59e-41fa-8cf0-57a8f4ce7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "_classifier_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921f4cb-d352-4ce5-9174-4d3c770e34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1 / ((2 * np.pi) ** (20 / 2))\n",
    "coeff * torch.exp(-0.5 * distances[0]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97fba00-ca16-47d1-970c-5bb31a1a731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls._empp.double() @ torch.exp(-0.5 * distances[0]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42dd56-c89b-4866-96c0-416b16696e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sort(torch.argmax(cls._empp, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34498736-dba2-4bf9-b3d7-f95671464b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls._classifier.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c498e-9682-41e5-8bb2-7ba2c45e36b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cls._empp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b70ea0-001d-47ea-b8c4-a3368549cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "peepholes = Peepholes(classifier=cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f518d6-8df4-4275-84a7-1b87b2266846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d789de-29ad-4c7c-a0b3-5da6bff9287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peepholes.get_peepholes(dataloader=ph_dl['val'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa150398-4593-49e8-bb5c-2d9a7729a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = peepholes._phs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6264e66b-5ac2-4c79-b4f5-689cc0775ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e12629-956f-400e-95b5-5dfd6d7c393c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.sum(torch.argmax(preds, axis=1)==true_labels['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975f45f-bd0e-45eb-99b5-01fd467425cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [5, ]\n",
    "n_list = [10, 20, 30, 50]\n",
    "\n",
    "df = evaluate_gmm(layer_list, n_list, k_list, n_classes, ph_dl, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756dfc7-56cc-4a51-82b4-ded5dd39cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = ['features.14', 'features.24', 'features.28', 'classifier.0', 'classifier.3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b5872c-e4fb-466e-9a0f-3c7bfcae0229",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_list = [5, 9, 50]\n",
    "n_list = [50, 100, 150] #[10, 20, 30, 50, 70]\n",
    "\n",
    "df_k, preds_k = evaluate_kmeans(layer_list, n_list, k_list, n_classes, ph_dl, true_labels, return_preds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3694ea-7e0c-415b-9008-b0471d5b83e7",
   "metadata": {},
   "source": [
    "### GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5c2f69-2fe8-4063-8fe5-95e5fb51cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10 if dataset=='CIFAR10' else 100\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe0d096-f6b3-4931-9a4c-e2a18cbb029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'classifier.0'\n",
    "n_clusters = 100\n",
    "peep_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d2a2a6-1cff-4675-bf1e-0c842eee2348",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_cv = trim_corevectors\n",
    "parser_kwargs = {'layer': layer, 'peep_size':peep_size}\n",
    "cls_kwargs = {'random_state': 42, 'n_init':n_classes, 'max_iter':500} \n",
    "\n",
    "cls = GMM(\n",
    "        nl_classifier = n_clusters,\n",
    "        nl_model = n_classes,\n",
    "        parser = parser_cv,\n",
    "        parser_kwargs = parser_kwargs,\n",
    "        cls_kwargs = cls_kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0cc1c5-097b-4285-9fc5-bcf933a4aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.fit(dataloader = ph_dl['train'], verbose=True)\n",
    "cls.compute_empirical_posteriors(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e8f6e-f1b6-461d-a0b5-f4d6410b5d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.classifier_probabilities(dataloader=ph_dl['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f568a-2e6e-49c9-bc15-92c98ab56e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls._classifier_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30529df-445a-4d3a-be66-ce28405d14c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_dl = ph_dl['val']\n",
    "n_samples = len(_dl.dataset)\n",
    "bs = _dl.batch_size\n",
    "\n",
    "# Allocate predict probs\n",
    "_classifier_probs = torch.zeros(n_samples, 100)\n",
    "\n",
    "for bn, batch in enumerate(tqdm(_dl)):\n",
    "    data = cls.parser(data = batch, **cls.parser_kwargs)\n",
    "    n_in = len(data)\n",
    "\n",
    "    probs = torch.tensor(cls._classifier.predict_proba(data))\n",
    "    _classifier_probs[bn*bs:bn*bs+n_in] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b1560b-42e7-4994-a808-0a659c339d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_classifier_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8350a749-c533-454f-be39-edaa4d8fbb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 1 / ((2 * np.pi) ** (20 / 2))\n",
    "coeff * torch.exp(-0.5 * distances[0]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eb12c0-061d-444e-a82c-e5a06386117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls._empp.double() @ torch.exp(-0.5 * distances[0]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e31bc0-e599-4b93-ad00-5d8b6b40170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sort(torch.argmax(cls._empp, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c61241-a42f-48c4-a134-665f9c1e48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls._classifier.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66cb26b-d350-42a3-8728-3b7718fcb1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cls._empp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe9b7f1-3550-4bea-a670-7ecf17113e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peepholes = Peepholes(classifier=cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73f9f6-9408-4172-a5ed-fd2a17523924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff641de-1725-4dd3-b11a-438b07211a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "peepholes.get_peepholes(dataloader=ph_dl['val'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1903886-324d-4cbf-9258-4d883dcf6d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = peepholes._phs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a0784e-2140-437b-aa15-0988408fa915",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8574a7-0fd7-404d-88f6-14987e19a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(torch.argmax(preds, axis=1)==true_labels['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f48b2c-49d9-40a1-810f-3ff48d79a77a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "k_list = [5, ]\n",
    "n_list = [10, 20, 30, 50]\n",
    "\n",
    "df = evaluate_gmm(layer_list, n_list, k_list, n_classes, ph_dl, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d91a253-48b2-4567-bbfd-e8d57c7b1626",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = ['features.14', 'features.24', 'features.28', 'classifier.0', 'classifier.3'] # CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f26d9-7961-4876-a542-afa399be5978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_list = [5, 9, 50, 100]\n",
    "n_list = [100, 150, 200] #[10, 20, 30, 50, 70]\n",
    "\n",
    "df_g, preds_g = evaluate_gmm(layer_list, n_list, k_list, n_classes, ph_dl, true_labels, return_preds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52472f7f-fe4b-4743-8a5e-3602c4fcc331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c539e088-620f-4bf0-9799-b0cc796ca3c2",
   "metadata": {},
   "source": [
    "### results with CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc995690-b940-4481-a7d3-489e5b61c287",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_k.unstack(level='layer').unstack(level='peep_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5909eea4-0586-4543-94b3-adb5e5667cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_k.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c3717d-e800-4357-89a6-837342832aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_k.xs(('classifier.3', 150, 50)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bca9be2-a9aa-4c60-abd5-ef3aabb6f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37caa615-8c0e-4edc-9a81-0b15fb892aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(x==tl)/len(tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eff675-8095-4445-bc40-de56a0e50cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = np.array(true_labels['val'])[decisions['val']]\n",
    "true_count = Counter(tl)\n",
    "true_unique_labels = list(true_count.keys())\n",
    "true_frequencies = list(true_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534393f8-6f50-45f1-8f40-3e31d3461b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e80aa-3fa7-46cb-b7ed-480839f0e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(preds_k.xs(('features.14', 100, 5)).values[0])[decisions['val']]\n",
    "\n",
    "label_counts = Counter(x)\n",
    "\n",
    "unique_labels = list(label_counts.keys())\n",
    "frequencies = list(label_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e2c00-209f-4e33-ace2-2234a1c896c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 150 # fix n_clusters\n",
    "algo = 'KMeans'\n",
    "\n",
    "fig, axs = plt.subplots(len(layer_list), len(k_list), sharex=True, figsize=(12, 12))\n",
    "\n",
    "title_ = f'Pred vs True labels - clustering={algo}, n_clusters={n}, dataset={dataset}'\n",
    "fig.suptitle(title_)\n",
    "\n",
    "for i, layer in enumerate(layer_list):\n",
    "    axs[i, 0].set_ylabel(layer)\n",
    "    for j, k in enumerate(k_list):\n",
    "\n",
    "        axs[0, j].set_title(f'corevector_size={k}')\n",
    "\n",
    "        x = np.array(preds_k.xs((layer, n, k)).values[0])[decisions['val']]\n",
    "\n",
    "        label_counts = Counter(x)\n",
    "        \n",
    "        unique_labels = list(label_counts.keys())\n",
    "        frequencies = list(label_counts.values())\n",
    "\n",
    "        acc_correct = np.sum(x==tl)/len(tl)\n",
    "\n",
    "        axs[i, j].bar(unique_labels, frequencies, alpha=0.5, label=f'acc={acc_correct:.4f}')\n",
    "        axs[i, j].bar(true_unique_labels, true_frequencies, alpha=0.5)\n",
    "\n",
    "        axs[i, j].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(title_+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce2a09-0a92-42c1-9adc-1e27a6aa84b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afcedcbf-4e70-41bc-a806-1dab5757aadf",
   "metadata": {},
   "source": [
    "### results with CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a90c21b-567e-4a07-b8d7-e6ed74f804cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.unstack(level='layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602780e2-46f9-413a-8164-c9247abda608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('seed=42')\n",
    "df_k.unstack(level='layer').unstack(level='peep_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7c9d9-ffbb-4b34-b951-34787190de8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Random seeds')\n",
    "df_k2.unstack(level='layer').unstack(level='peep_size')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4bb04-31a4-476b-9e16-ce1d53e588ee",
   "metadata": {},
   "source": [
    "## Caccia le immaginine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b399d9-7b91-445e-9355-30b760857b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clustering.utils import get_unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c496f73d-5921-42fa-b736-d11744d526ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_path = '/srv/newpenny/XAI/generated_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92998484-ff2f-4422-9f9b-59403efc1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'gmm'\n",
    "dataset = 'CIFAR100'\n",
    "dnn_model = 'vgg16'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ec23c-f4e2-4b05-914d-a6b54c64fc8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load peephole scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5d25db-d102-4b4a-b706-dd649f13c9a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algorithm = 'gmm'\n",
    "# init the peephole container if not existing\n",
    "new_peep_dir = 'clustering/peepholes' \n",
    "new_peep_path = os.path.join(abs_path, new_peep_dir) \n",
    "new_peep_tensor_dict_path = os.path.join(new_peep_path, f'algorithm={algorithm}_dataset={dataset}_dnn={dnn_model}.memmap')\n",
    "\n",
    "if os.path.exists(new_peep_tensor_dict_path):\n",
    "    print('New peepholes results already present')\n",
    "    peephole_scores = TensorDict.load_memmap(new_peep_tensor_dict_path)\n",
    "else:\n",
    "    print('Initializing peephole container')\n",
    "    peephole_scores = TensorDict({}, batch_size=[])\n",
    "\n",
    "ps_gmm = peephole_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97db15-659e-41dc-ba60-c130adaa7fd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algorithm = 'kmeans'\n",
    "# init the peephole container if not existing\n",
    "new_peep_dir = 'clustering/peepholes' \n",
    "new_peep_path = os.path.join(abs_path, new_peep_dir) \n",
    "new_peep_tensor_dict_path = os.path.join(new_peep_path, f'algorithm={algorithm}_dataset={dataset}_dnn={dnn_model}.memmap')\n",
    "\n",
    "if os.path.exists(new_peep_tensor_dict_path):\n",
    "    print('New peepholes results already present')\n",
    "    peephole_scores = TensorDict.load_memmap(new_peep_tensor_dict_path)\n",
    "else:\n",
    "    print('Initializing peephole container')\n",
    "    peephole_scores = TensorDict({}, batch_size=[])\n",
    "\n",
    "ps_kmeans = peephole_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438891f7-571b-4691-9b95-039444bc99d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {'results_tensordict' : peephole_scores}\n",
    "hps = get_unique_values(**scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e11ad3-8613-42c7-a153-33d5782e5138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_list, n_list, splits, layers = hps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea78729-0d72-48ed-9e11-c02907a705d7",
   "metadata": {},
   "source": [
    "### Get true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9513af1-14fe-4796-925c-e98878c92a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788e80f-d4b9-4ae3-95cd-a0e023846de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9fb38b-8cb7-4f7f-ac28-4fce926c3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = {}\n",
    "decisions = {}\n",
    "\n",
    "for split in splits:\n",
    "    true_labels[split] = []\n",
    "    decisions[split] = []\n",
    "\n",
    "    for batch in tqdm(ph_dl[split]):\n",
    "        peepholes = batch['coreVectors']\n",
    "        labels = batch['label']\n",
    "        decision_results = batch['result']\n",
    "\n",
    "        true_labels[split].append(labels)\n",
    "        decisions[split].append(decision_results.bool())\n",
    "\n",
    "    true_labels[split] = torch.cat(true_labels[split], dim=0)\n",
    "    decisions[split] = torch.cat(decisions[split], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69527aa-427e-4b1b-8464-8bf64cf87d2a",
   "metadata": {},
   "source": [
    "### Check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882bc04-7428-4667-8396-73ba82ce318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ffee81-3f5a-4564-9468-9899ed4fc877",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl==lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c28034-0104-4b9c-aa35-7141ca65fe51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = {}\n",
    "idxs = {}\n",
    "\n",
    "for k in tqdm(k_list):\n",
    "    for n in n_list:\n",
    "        for split in splits:\n",
    "            tl = true_labels[split]\n",
    "            \n",
    "            for l in layers:\n",
    "                lp = torch.argmax(peephole_scores[k][n][split][l], axis=1)\n",
    "                res = np.float64(torch.sum(tl==lp) / len(tl))\n",
    "\n",
    "                acc[(k, n, split, l)] = res\n",
    "\n",
    "                eq_idx = torch.nonzero(tl==lp, as_tuple=False).squeeze()\n",
    "                idxs[(k, n, split, l)] = eq_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2c3b26-ee9c-4c8a-8179-964d79ded4c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# solo val \n",
    "split = 'val'\n",
    "tl = true_labels[split]\n",
    "peephole_scores = ps_gmm\n",
    "acc_gmm = {}\n",
    "top_acc_gmm = {}\n",
    "\n",
    "idxs_gmm = {}\n",
    "top_idxs_gmm = {}\n",
    "\n",
    "for k in tqdm(k_list):\n",
    "    for n in n_list:            \n",
    "            for l in layers:\n",
    "                scores = ps_gmm[k][n][split][l]\n",
    "                \n",
    "                lp = torch.argmax(scores, axis=1)\n",
    "                # std acc\n",
    "                res = np.float64(torch.sum(tl==lp) / len(tl))\n",
    "                acc_gmm[(k, n, l)] = res\n",
    "                eq_idx = torch.nonzero(tl==lp, as_tuple=False).squeeze()\n",
    "                idxs_gmm[(k, n, l)] = eq_idx\n",
    "                # top acc\n",
    "                _, top_idxs = torch.topk(scores, 10, dim=1)\n",
    "                top_correct = (tl.unsqueeze(1)==top_idxs).any(dim=1)\n",
    "                top_acc = top_correct.float().mean().item()\n",
    "                top_idxs_gmm[(k, n, l)] = top_acc\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b601ca8-d6be-49a7-a10b-59f7f8e66d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8dcab-4d97-45b5-9850-30604f49254b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddda4a0-6ea0-4f19-900d-30ad3f56c14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c357468-6ca0-4634-b177-ceac3686f90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_idxs_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a9577-02e5-435f-9568-80d43b4a53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solo val \n",
    "split = 'val'\n",
    "tl = true_labels[split]\n",
    "peephole_scores = ps_kmeans\n",
    "acc_kmeans = {}\n",
    "idxs_kmeans = {}\n",
    "\n",
    "for k in tqdm(k_list):\n",
    "    for n in n_list:            \n",
    "            for l in layers:\n",
    "                scores = ps_kmeans[k][n][split][l]\n",
    "                lp = torch.argmax(scores, axis=1)\n",
    "                res = np.float64(torch.sum(tl==lp) / len(tl))\n",
    "\n",
    "                acc_kmeans[(k, n, l)] = res\n",
    "\n",
    "                eq_idx = torch.nonzero(tl==lp, as_tuple=False).squeeze()\n",
    "                idxs_kmeans[(k, n, l)] = eq_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc5c1c-0868-4fc5-91b5-efcf550dede9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_labels['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f45e7-2332-4c42-9c9b-89115e97cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5022b7-d1b8-45b1-a1a8-947a4717eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    'features.7',\n",
    "    'features.14',\n",
    "     'features.24',\n",
    "     'features.26',\n",
    "     'features.28',\n",
    "     'classifier.0',\n",
    "     'classifier.3', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3871a22-60b6-4a49-baa2-69504590fb5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85996918-fc3c-40c0-8b7d-46af12adae1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a41bc34-f651-4f7a-930b-f4a137da8e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903b62a-f288-41a3-ad22-b3a27aeda451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7cfca7-e01f-478b-8c8e-7675794c1597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159cc864-618b-4ebc-bf10-54b3dfed81e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_prova = 9478\n",
    "idx_prova = 151\n",
    "k_ = k_list[-1]\n",
    "n_ = n_list[2]\n",
    "split = 'val'\n",
    "\n",
    "t1 = []\n",
    "t2 = []\n",
    "t3 = []\n",
    "\n",
    "for l in layers:\n",
    "    # tensor = ps_kmeans[k_][n_][split][l][idx_prova]\n",
    "    # if idx_prova in idxs_kmeans[(k_, n_, l)]:\n",
    "    tensor = ps_gmm[k_][n_][split][l][idx_prova]\n",
    "    if idx_prova in idxs_gmm[(k_, n_, l)]:\n",
    "        print(f'{idx_prova} present for {l}')\n",
    "    t1.append(tensor)\n",
    "    t2.append(tensor**0.5)\n",
    "    t3.append(tensor**2)\n",
    "\n",
    "pc1 = torch.stack(t1)\n",
    "pc2 = torch.stack(t2)\n",
    "pc3 = torch.stack(t3)\n",
    "\n",
    "# pc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa6cc2f-3edd-445f-b43c-612c320922e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 6, sharey=True, figsize=(8, 8))\n",
    "\n",
    "fig.suptitle(f\"Combined peepholes; instance_idx={idx_prova}, true_label={true_labels['val'][idx_prova]}\")\n",
    "\n",
    "axs[0].imshow(pc1.T)\n",
    "axs[0].set_title('exp=1')\n",
    "axs[1].imshow(torch.sum(pc1, axis=0).reshape(-1, 1))\n",
    "\n",
    "axs[2].imshow(pc2.T)\n",
    "axs[2].set_title('exp=0.5')\n",
    "axs[3].imshow(torch.sum(pc2, axis=0).reshape(-1, 1))\n",
    "\n",
    "axs[4].imshow(pc3.T)\n",
    "axs[4].set_title('exp=2')\n",
    "axs[5].imshow(torch.sum(pc3, axis=0).reshape(-1, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f7c3c-865e-47dd-ae2c-0606d385d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels['val'][idx_prova]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb011fa9-09f9-4004-b78b-ce18c8c42707",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_kmeans[(k, n, layers[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9518826-e609-4b2b-bbc4-7f2b4e7078e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "k, n, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15bf92-0bfd-40d1-96ff-436395bcc329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57877301-b569-4b19-ae60-5b8d85336723",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_ = k_list[0]\n",
    "n_ = n_list[0]\n",
    "l_ = 'classifier.0'\n",
    "\n",
    "idx_counter = Counter()\n",
    "\n",
    "for key, indices in idxs_kmeans.items():\n",
    "    k, n, l = key\n",
    "    # if k==k_ and n==n_:\n",
    "    if l==l_:\n",
    "        idx_counter.update(list(indices))\n",
    "\n",
    "num_configs = sum(1 for key in idxs_kmeans.keys() if key[2]==l_)#if key[0]==k_ and key[1]==n_)\n",
    "\n",
    "common_idx = [index for index, count in idx_counter.items() if count==num_configs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e073d09-8d6f-458b-9eea-cacc9cf49417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(idxs_kmeans[(k_, n_, l_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb0b42-b90a-497f-ad76-ad1b8c9a1708",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(idxs_gmm[(k_, n_, l_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb15f224-7d6a-4aee-a70c-3f414c149493",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = idxs_kmeans[(k_, n_, layers[0])]\n",
    "b = idxs_kmeans[(k_, n_, layers[1])]\n",
    "\n",
    "cmn = []\n",
    "\n",
    "for lb in a:\n",
    "    if lb in b:\n",
    "        cmn.append(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0116bc-b7e1-4bba-8fa8-e992aae7d3f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(cmn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b0301-72c2-400f-835a-14db0fb6ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = idxs_kmeans[(k_, n_, layers[2])]\n",
    "b = idxs_kmeans[(k_, n_, layers[3])]\n",
    "\n",
    "cmn = []\n",
    "\n",
    "for lb in a:\n",
    "    if lb in b:\n",
    "        cmn.append(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acdf269-a547-4c14-8ab2-214abc826468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4db80-a2c6-4add-a51d-eb75e6de19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = idxs_kmeans[(k_, n_, layers[3])]\n",
    "b = idxs_kmeans[(k_, n_, layers[4])]\n",
    "\n",
    "cmn = []\n",
    "\n",
    "for lb in a:\n",
    "    if lb in b:\n",
    "        cmn.append(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62454193-6fc1-44ab-b183-b552213bc462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a59e4-a38d-4cfc-8763-934990517ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = idxs_kmeans[(k_, n_, layers[4])]\n",
    "b = idxs_kmeans[(k_, n_, layers[5])]\n",
    "\n",
    "cmn = []\n",
    "\n",
    "for lb in a:\n",
    "    if lb in b:\n",
    "        cmn.append(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904f154-db8f-4a17-a87b-127d496769b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545dd6e8-bbef-4982-b288-8701a3e6482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = idxs_kmeans[(k_, n_, layers[4])]\n",
    "b = idxs_kmeans[(k_, n_, layers[6])]\n",
    "\n",
    "cmn = []\n",
    "\n",
    "for lb in a:\n",
    "    if lb in b:\n",
    "        cmn.append(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4081f170-85a5-4002-ac76-e7321fb059a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60ae951-381a-4aab-83f4-e7cf469559b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975a1b0-48f4-40e7-9134-1eea7f3b6538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3cc4d-072f-42ab-9c8a-34c4b70bff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_series = pd.Series(acc_kmeans)\n",
    "acc_df_kmeans = acc_series.to_frame(name='accuracy')\n",
    "# acc_df.index.names = ['k', 'n', 'split', 'l']\n",
    "acc_df_kmeans.index.names = ['k', 'n', 'l']\n",
    "acc_df_kmeans = acc_df_kmeans.unstack(level='l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f3bef-99ed-4d8d-8cde-50c086b0e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxmax_kmeans = pd.Series(acc_df_kmeans.idxmax())\n",
    "idxmax_kmeans['accuracy', 'classifier.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec62bd6-ae1d-4b5b-9428-8409b243b1b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_df_kmeans.style.background_gradient(cmap='YlOrRd', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8291c674-2c6b-4900-bb86-d01617a7e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_series = pd.Series(acc_gmm)\n",
    "acc_df_gmm = acc_series.to_frame(name='accuracy')\n",
    "# acc_df.index.names = ['k', 'n', 'split', 'l']\n",
    "acc_df_gmm.index.names = ['k', 'n', 'l']\n",
    "acc_df_gmm = acc_df_gmm.unstack(level='l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bdad25-d775-4f84-9973-7af2b56f638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxmax_gmm = pd.Series(acc_df_gmm.idxmax())\n",
    "idxmax_gmm['accuracy', 'classifier.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14757d44-1a1c-43c2-a6e1-047df52ae194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e47e52-39c6-40eb-a1ab-5d38f287c27d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_df_gmm.style.background_gradient(cmap='YlOrRd', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b32e51-f9e0-4e1c-8299-c6c38d48e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df_unstacked.loc[('100', '100')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d06aa-0400-4a08-be17-1f64b91e546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "peephole_scores[k][n]['val'][l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efe75e3-5228-4d22-9e96-eda26065b131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
