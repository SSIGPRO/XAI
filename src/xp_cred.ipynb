{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6334640-c5d0-4da9-9c09-10aa5e29e4c2",
   "metadata": {},
   "source": [
    "## Experimenting DkNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d8af8f-cbaf-499c-a5db-11baf9a43920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:5 device\n",
      "dataset: CIFAR100\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x7f1155fefe60>,\n",
       " 'val': <torch.utils.data.dataloader.DataLoader at 0x7f11560f1790>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x7f115df98aa0>}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# python stuff\n",
    "\n",
    "from pathlib import Path as Path\n",
    "from numpy.random import randint\n",
    "\n",
    "# Our stuff\n",
    "from datasets.cifar import Cifar\n",
    "from models.model_wrap import ModelWrap\n",
    "\n",
    "# from credibility import get_credibility\n",
    "\n",
    "# torch stuff\n",
    "import torch\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from peepholes.peepholes import Peepholes\n",
    "from peepholes.svd_peepholes import peep_matrices_from_svds as parser_fn\n",
    "from credibility.DkNN import NearestNeighbor, DkNN\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "cuda_index = torch.cuda.device_count() - 3\n",
    "device = torch.device(f\"cuda:{cuda_index}\" if use_cuda else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "#--------------------------------\n",
    "# Dataset \n",
    "#--------------------------------\n",
    "# model parameters\n",
    "dataset = 'CIFAR100' \n",
    "seed = 29\n",
    "bs = 64\n",
    "data_path = '/srv/newpenny/XAI/LM/data/CIFAR100'\n",
    "\n",
    "ds = Cifar(dataset=dataset, data_path=data_path)\n",
    "ds.load_data(\n",
    "        batch_size = bs,\n",
    "        data_kwargs = {'num_workers': 4, 'pin_memory': True},\n",
    "        seed = seed,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a649356e-0e50-4155-85ea-28a9fc4fd3c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenzocapelli/repos/XAI/src/models/model_wrap.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self._checkpoint = torch.load(file, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------\n",
      "checkpoint\n",
      "-----------------\n",
      "state_dict keys: \n",
      " odict_keys(['features.0.weight', 'features.0.bias', 'features.2.weight', 'features.2.bias', 'features.5.weight', 'features.5.bias', 'features.7.weight', 'features.7.bias', 'features.10.weight', 'features.10.bias', 'features.12.weight', 'features.12.bias', 'features.14.weight', 'features.14.bias', 'features.17.weight', 'features.17.bias', 'features.19.weight', 'features.19.bias', 'features.21.weight', 'features.21.bias', 'features.24.weight', 'features.24.bias', 'features.26.weight', 'features.26.bias', 'features.28.weight', 'features.28.bias', 'classifier.0.weight', 'classifier.0.bias', 'classifier.3.weight', 'classifier.3.bias', 'classifier.6.weight', 'classifier.6.bias']) \n",
      "\n",
      "train_loss 1.4151224618911744\n",
      "val_loss 0.8791644280883157\n",
      "train_accuracy 62.295\n",
      "val_accuracy 74.96000000000001\n",
      "epoch 59\n",
      "batch_size 64\n",
      "lr 0.001\n",
      "-----------------\n",
      "\n",
      "target layers:  {'classifier.0': Linear(in_features=25088, out_features=4096, bias=True)}\n",
      "File /home/lorenzocapelli/repos/XAI/src/../data/svdsBanana/svdsBatata exists. Loading from disk.\n",
      "Layers to compute SVDs:  []\n",
      "saving /home/lorenzocapelli/repos/XAI/src/../data/svdsBanana/svdsBatata\n",
      "svd shapes:  classifier.0 torch.Size([4096, 25089])\n",
      "svd shapes:  classifier.3 torch.Size([4096, 4097])\n",
      "svd shapes:  features.28 torch.Size([300, 131073])\n",
      "\n",
      " ---- Getting data from train\n",
      "\n",
      "File /home/lorenzocapelli/repos/XAI/src/../data/peepholes/peepholes.train exists. Loading from disk.\n",
      "loaded n_samples:  40000\n",
      "\n",
      " ---- Getting data from val\n",
      "\n",
      "File /home/lorenzocapelli/repos/XAI/src/../data/peepholes/peepholes.val exists. Loading from disk.\n",
      "loaded n_samples:  10000\n",
      "\n",
      " ---- Getting data from test\n",
      "\n",
      "File /home/lorenzocapelli/repos/XAI/src/../data/peepholes/peepholes.test exists. Loading from disk.\n",
      "loaded n_samples:  10000\n",
      "\n",
      " ---- Getting activations for train\n",
      "\n",
      "In activations exist.\n",
      "Out activations exist.\n",
      "Layers to save:  []\n",
      "No new activations for train, skipping\n",
      "\n",
      " ---- Getting activations for val\n",
      "\n",
      "In activations exist.\n",
      "Out activations exist.\n",
      "Layers to save:  []\n",
      "No new activations for val, skipping\n",
      "\n",
      " ---- Getting activations for test\n",
      "\n",
      "In activations exist.\n",
      "Out activations exist.\n",
      "Layers to save:  []\n",
      "No new activations for test, skipping\n",
      "\n",
      " ---- Getting peepholes for train\n",
      "\n",
      "Peepholes TensorDict exists.\n",
      "peep_m shape:  torch.Size([4096, 25089])\n",
      "Layers to save:  []\n",
      "No new peepholes for train, skipping\n",
      "\n",
      " ---- Getting peepholes for val\n",
      "\n",
      "Peepholes TensorDict exists.\n",
      "peep_m shape:  torch.Size([4096, 25089])\n",
      "Layers to save:  []\n",
      "No new peepholes for val, skipping\n",
      "\n",
      " ---- Getting peepholes for test\n",
      "\n",
      "Peepholes TensorDict exists.\n",
      "peep_m shape:  torch.Size([4096, 25089])\n",
      "Layers to save:  []\n",
      "No new peepholes for test, skipping\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------\n",
    "# Model \n",
    "#--------------------------------\n",
    "pretrained = True\n",
    "model_dir = '/srv/newpenny/XAI/LM/models'\n",
    "model_name = f'vgg16_pretrained={pretrained}_dataset={dataset}-'\\\n",
    "f'augmented_policy=CIFAR10_bs={bs}_seed={seed}.pth'\n",
    "\n",
    "nn = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "in_features = 4096\n",
    "num_classes = len(ds.get_classes()) \n",
    "nn.classifier[-1] = torch.nn.Linear(in_features, num_classes)\n",
    "model = ModelWrap(device=device)\n",
    "model.set_model(model=nn, path=model_dir, name=model_name, verbose=True)\n",
    "\n",
    "layers_dict = {'classifier': [0]}\n",
    "              \n",
    "model.set_target_layers(target_layers=layers_dict, verbose=True)\n",
    "print('target layers: ', model.get_target_layers()) \n",
    "\n",
    "direction = {'save_input':True, 'save_output':False}\n",
    "model.add_hooks(**direction, verbose=False) \n",
    "\n",
    "dry_img, _ = ds._train_ds.dataset[0]\n",
    "dry_img = dry_img.reshape((1,)+dry_img.shape)\n",
    "model.dry_run(x=dry_img)\n",
    "\n",
    "#--------------------------------\n",
    "# SVDs \n",
    "#--------------------------------\n",
    "svds_path = Path.cwd()/'../data/svdsBanana'\n",
    "svds_name = 'svdsBatata' \n",
    "model.get_svds(model=model, path=svds_path, name=svds_name, verbose=True)\n",
    "for k in model._svds.keys():\n",
    "    print('svd shapes: ', k, model._svds[k]['Vh'].shape)\n",
    "#--------------------------------\n",
    "# Peepholes \n",
    "#--------------------------------\n",
    "phs_name = 'peepholes'\n",
    "phs_dir = Path.cwd()/'../data/peepholes'\n",
    "peepholes = Peepholes(\n",
    "        path = phs_dir,\n",
    "        name = phs_name,\n",
    "        )\n",
    "loaders = ds.get_dataset_loaders()\n",
    "# copy dataset to peepholes dataset\n",
    "peepholes.get_peep_dataset(\n",
    "        loaders = loaders,\n",
    "        verbose = True\n",
    "        ) \n",
    "\n",
    "peepholes.get_activations(\n",
    "        model=model,\n",
    "        loaders=loaders,\n",
    "        verbose=True\n",
    "        )\n",
    "\n",
    "peepholes.get_peepholes(\n",
    "        model = model,\n",
    "        peep_matrices = model._svds,\n",
    "        parser = parser_fn,\n",
    "        verbose = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c9a4d07-7635-4981-b323-7389a90f21d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dataloader for:  train\n",
      "creating dataloader for:  val\n",
      "creating dataloader for:  test\n"
     ]
    }
   ],
   "source": [
    "batch_dict = {key : value for key, value in peepholes._n_samples.items()}\n",
    "kwargs = {'batch_dict': batch_dict,\n",
    "          'verbose': True}\n",
    "ph_dl = peepholes.get_dataloaders(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bf1150f-57c7-403c-8812-b099360a558f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f1156007a10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ph_dl['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb6e2e-33bf-49ca-8a05-82c75eeb11f8",
   "metadata": {},
   "source": [
    "# Initialize DkNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a931b1e1-63ff-4d6d-92ce-c6b188b949b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = ds.config['num_classes']\n",
    "neighbors = 75\n",
    "percentage = {'train':5,\n",
    "               'val':1,\n",
    "               'test':1}\n",
    "\n",
    "verbose = True\n",
    "\n",
    "dknn_path = Path.cwd()/'../data/DkNN'\n",
    "dknn_name = 'DkNN' \n",
    "nb_tables = 200\n",
    "number_bits = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "902150e5-bb95-4709-9494-7f455ede9e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'model' : model,\n",
    "          'nb_classes' : nb_classes,\n",
    "          'neighbors' : neighbors,\n",
    "          'ph_dl' : ph_dl,\n",
    "          'percentage' : percentage, \n",
    "          'seed' : seed,\n",
    "          'verbose' : verbose,\n",
    "          'path' : dknn_path,\n",
    "          'name' : dknn_name,\n",
    "          'nearest_neighbor_backend' : NearestNeighbor.BACKEND.FALCONN,\n",
    "          'nb_tables' : nb_tables,\n",
    "          'number_bits' : number_bits,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7985d9df-1ba0-4d4a-ac4a-42f16c17f3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- DkNN init\n",
      "\n",
      "File /home/lorenzocapelli/repos/XAI/src/../data/DkNN/['classifier.0']/train_5/val_1 exists.\n"
     ]
    }
   ],
   "source": [
    "dknn = DkNN(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcd3ed41-7bd8-4137-a2db-4cc1b82351b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- DkNN calibrate\n",
      "\n",
      "## Starting calibration of DkNN\n"
     ]
    }
   ],
   "source": [
    "dknn.calibrate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b1701ea-e539-48a0-92a6-a6454689cfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- DkNN predict\n",
      "\n",
      "\n",
      " ---- Getting scores for train\n",
      "\n",
      "Nonconformity calculated\n",
      "Saving train to /home/lorenzocapelli/repos/XAI/src/../data/DkNN/['classifier.0']/train_5/val_1/train.\n",
      "\n",
      " ---- Getting scores for val\n",
      "\n",
      "Nonconformity calculated\n",
      "Saving val to /home/lorenzocapelli/repos/XAI/src/../data/DkNN/['classifier.0']/train_5/val_1/val.\n",
      "\n",
      " ---- Getting scores for test\n",
      "\n",
      "Nonconformity calculated\n",
      "Saving test to /home/lorenzocapelli/repos/XAI/src/../data/DkNN/['classifier.0']/train_5/val_1/test.\n"
     ]
    }
   ],
   "source": [
    "dknn.fprop('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d37d1d3-bd63-4984-b3a8-ff8d09f7f3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        test: TensorDict(\n",
       "            fields={\n",
       "                confs: MemoryMappedTensor(shape=torch.Size([10000]), device=cpu, dtype=torch.float32, is_shared=True),\n",
       "                creds: MemoryMappedTensor(shape=torch.Size([10000]), device=cpu, dtype=torch.float32, is_shared=True),\n",
       "                p-value: MemoryMappedTensor(shape=torch.Size([10000, 100]), device=cpu, dtype=torch.float32, is_shared=True),\n",
       "                preds_knn: MemoryMappedTensor(shape=torch.Size([10000]), device=cpu, dtype=torch.int32, is_shared=True)},\n",
       "            batch_size=torch.Size([10000]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        train: TensorDict(\n",
       "            fields={\n",
       "                confs: MemoryMappedTensor(shape=torch.Size([40000]), device=cpu, dtype=torch.float32, is_shared=True),\n",
       "                creds: MemoryMappedTensor(shape=torch.Size([40000]), device=cpu, dtype=torch.float32, is_shared=True),\n",
       "                p-value: MemoryMappedTensor(shape=torch.Size([40000, 100]), device=cpu, dtype=torch.float32, is_shared=True),\n",
       "                preds_knn: MemoryMappedTensor(shape=torch.Size([40000]), device=cpu, dtype=torch.int32, is_shared=True)},\n",
       "            batch_size=torch.Size([40000]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        val: TensorDict(\n",
       "            fields={\n",
       "                confs: MemoryMappedTensor(shape=torch.Size([10000]), device=cpu, dtype=torch.float32, is_shared=True),\n",
       "                creds: MemoryMappedTensor(shape=torch.Size([10000]), device=cpu, dtype=torch.float32, is_shared=True),\n",
       "                p-value: MemoryMappedTensor(shape=torch.Size([10000, 100]), device=cpu, dtype=torch.float32, is_shared=True),\n",
       "                preds_knn: MemoryMappedTensor(shape=torch.Size([10000]), device=cpu, dtype=torch.int32, is_shared=True)},\n",
       "            batch_size=torch.Size([10000]),\n",
       "            device=cpu,\n",
       "            is_shared=False)},\n",
       "    batch_size=torch.Size([]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dknn.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1da473-e01e-4b86-9ea5-8b99aba0de0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f8f11-a464-4f21-929e-bc6b9d59388c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
